{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test or Prod\n",
    "env = 'prod'  #'prod'\n",
    "\n",
    "if env == 'test':\n",
    "    RESULTS_WORKBOOK = {'sheet_id' : '1HAqjdiUgvDK-0sQjsg-TYEIxJf2q0WD7X8XUs_pPOx0',\n",
    "                     'range_names': ['staged_tweets','twitter_authors']}   \n",
    "    stop_after = 1\n",
    "    sleeptime = 1\n",
    "    search_languages = ['en']\n",
    "    control_input = 'googlesheets'\n",
    "elif env == 'prod':\n",
    "    RESULTS_WORKBOOK = {'sheet_id' : '1_6-O1D7UtbA4PiDNr71Fm-kA_OJk8S7RBctkGjZRIVk',\n",
    "                     'range_names': ['tweet_list','staged_tweets','twitter_authors']}\n",
    "    stop_after = -1\n",
    "    sleeptime = 36000 #10 hour sleep time\n",
    "    search_languages = ['en']\n",
    "    control_input = 'googlesheets'\n",
    "else:\n",
    "    print(\"\"\"'valid env values are 'prod' or 'test'\"\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "\n",
    "#https://www.thepythoncode.com/article/translate-text-in-python\n",
    "from googletrans import Translator, constants\n",
    "from pprint import pprint\n",
    "\n",
    "#to connect to google\n",
    "from google.oauth2 import service_account\n",
    "import pygsheets\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "import jmespath\n",
    "import urlexpander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_twitter_version = 'v2022-11-26'\n",
    "runDate = datetime.today().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "#twitter variables\n",
    "API_Key = os.environ.get(\"API_KEY\")\n",
    "API_Key_Secret = os.environ.get(\"API_KEY_SECRET\")\n",
    "Bearer_Token = os.environ.get(\"BEARER_TOKEN\")\n",
    "bearer = 'bearer ' + Bearer_Token\n",
    "\n",
    "\n",
    "#google variables\n",
    "SERVICE_ACCOUNT_FILE = os.environ.get(\"GOOGLE_SERVICE_ACCOUNT_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheets Variables\n",
    "# variables that shouldn't change by person\n",
    "\n",
    "SEARCH_WORKBOOK = {'sheet_id' : '1QjvZOnkCJM-BcRvMlP0XsN0hJeanKPiK7_e37uJiKek',\n",
    "                    'range_names': ['twitter_keywords','stop_phrases']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_google_sheets_connector():\n",
    "    # Set up Google Credentials\n",
    "    #SERVICE_ACCOUNT_FILE = google_service_account_file\n",
    "\n",
    "    SCOPES = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    # connect\n",
    "    gc = pygsheets.authorize(credentials=service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, \n",
    "        scopes=SCOPES))\n",
    "    \n",
    "    return gc\n",
    "\n",
    "gc = instantiate_google_sheets_connector()\n",
    "\n",
    "#load to workbook objects\n",
    "search_workbook = gc.open_by_key(SEARCH_WORKBOOK['sheet_id'])\n",
    "results_workbook = gc.open_by_key(RESULTS_WORKBOOK['sheet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate read-only dataframes\n",
    "keywords = search_workbook[0]\n",
    "#df_r_keywords = pd.DataFrame(keywords.get_all_records())\n",
    "\n",
    "stops = search_workbook[1]\n",
    "#df_r_stop_phrases = pd.DataFrame(stop_phrases.get_all_records())\n",
    "\n",
    "#generate updatable dataframes\n",
    "tweet_list = results_workbook[0]\n",
    "#df_u_tweet_list = pd.DataFrame(tweet_list.get_all_records())\n",
    "\n",
    "staged_tweets = results_workbook[1]\n",
    "#df_u_staged_tweets = pd.DataFrame(staged_tweets.get_all_records())\n",
    "\n",
    "twitter_authors = results_workbook[2]\n",
    "global DF_AUTHORS\n",
    "DF_AUTHORS = pd.DataFrame(twitter_authors.get_all_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twitter_scanner:\n",
    "    def __init__(self, bearer, keywords):\n",
    "        self.bearer = bearer\n",
    "        self.base_api_url = 'https://api.twitter.com/2/tweets/search/recent?query='\n",
    "        self.fixed_args = '-is:retweet  has:links '\n",
    "        self.query_arg_suffix = '&max_results=100&tweet.fields=attachments,author_id,context_annotations,\\\n",
    "created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,\\\n",
    "referenced_tweets,source,text,withheld&expansions=referenced_tweets.id,geo.place_id'\n",
    "        self.refresh_scan_terms()\n",
    "\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.query_arg_suffix}'\n",
    "\n",
    "    def scan_for_search_terms(self,keywords):\n",
    "        print(f'scanning for {keywords}')\n",
    "        query_arg_prefix = self.fixed_args + keywords\n",
    "        query_arg_prefix = urllib.parse.quote(query_arg_prefix)\n",
    "        query = self.base_api_url + query_arg_prefix + self.query_arg_suffix\n",
    "        \n",
    "        response = requests.get(query, headers={\"Authorization\":bearer})\n",
    "        return(json.loads(response.text))\n",
    "        \n",
    "    \n",
    "    def scan_for_author(self,author):\n",
    "        print(f'scanning for {author}')\n",
    "        query_arg_prefix = urllib.parse.quote(f'-is:retweet  has:links from:{author}')   \n",
    "        query = self.base_api_url + query_arg_prefix + self.query_arg_suffix\n",
    "\n",
    "        \n",
    "        response = requests.get(query, headers={\"Authorization\":bearer})\n",
    "        return(json.loads(response.text))\n",
    "    \n",
    "    def refresh_scan_terms(self):\n",
    "        self.keywords = pd.DataFrame(keywords.get_all_records())\n",
    "        self.keywords.status = self.keywords.status.str.lower()\n",
    "        self.keywords = self.keywords[self.keywords.status == 'active']\n",
    "\n",
    "    \n",
    "    def scan_all_active_terms(self,num,sleep_time):\n",
    "        i = 0\n",
    "        while i != num:\n",
    "            i+=1\n",
    "            self.refresh_scan_terms()\n",
    "            key_terms = self.keywords.key_words[self.keywords.record_type == 'search_term'].tolist()\n",
    "            authors = self.keywords.key_words[self.keywords.record_type == 'twitter_user'].tolist()\n",
    "            \n",
    "            for t in key_terms:\n",
    "                res = self.scan_for_search_terms(t)\n",
    "                df = tf.tweets_to_df(res)\n",
    "                df['query_twitter_version'] = query_twitter_version\n",
    "                df['search_term'] = f'search_string:{t}'\n",
    "                ts.screen_tweets(df)\n",
    "\n",
    "            for a in authors:\n",
    "                res = self.scan_for_author(a)\n",
    "                df = tf.tweets_to_df(res)\n",
    "                df['query_twitter_version'] = query_twitter_version\n",
    "                df['search_term'] = f'author:{a}'               \n",
    "                ts.screen_tweets(df)            \n",
    "    \n",
    "            print(f'about to sleep for {sleep_time}, i is {i}')\n",
    "            time.sleep(sleep_time)\n",
    "            print('done')\n",
    "                  \n",
    "    \n",
    "tscanner = twitter_scanner(bearer, keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_screener:\n",
    "    def __init__(self, stops, staged_tweets):\n",
    "\n",
    "        self.staged_tweets = pd.DataFrame(staged_tweets.get_all_records())\n",
    "        self.stops = pd.DataFrame(stops.get_all_records())\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.stops}'\n",
    "\n",
    "    \n",
    "    def refresh_stops(self):\n",
    "        #print('refreshing critera')\n",
    "        self.stops = pd.DataFrame(stops.get_all_records())\n",
    "        self.stops = self.stops[self.stops.type == 'text']\n",
    "        print(self.stops)\n",
    "        \n",
    "    def refresh_saved_tweets(self):\n",
    "        #print('refreshing saved tweets')\n",
    "        self.staged_tweets = pd.DataFrame(staged_tweets.get_all_records())\n",
    "        \n",
    "    def screen_tweets(self, df_tweets):\n",
    "        self.refresh_saved_tweets()\n",
    "        #self.df_tweets = df_tweets\n",
    "        \n",
    "        #check to see if tweets have already been saved\n",
    "         \n",
    "        \n",
    "        #print(f'before checking for dupes {df_tweets.shape}')\n",
    "        df_tweets = df_tweets.assign(tweet_already_staged=df_tweets.tweet_id.isin(self.staged_tweets.tweet_id).astype(int))\n",
    "        df_x = df_tweets[df_tweets['tweet_already_staged'] == 0]\n",
    "        #print(f'after checking for dupes {df_x.shape}')\n",
    "        self.stage_tweets(df_x)\n",
    "        \n",
    "        return df_tweets\n",
    "    \n",
    "    def stage_tweets(self, df_tweets):\n",
    "        self.df_tweets = df_tweets\n",
    "        \n",
    "        #if anything remains, write to the tweet_list sheet\n",
    "        if len(df_tweets) > 0:\n",
    "            staged_tweets.append_table(df_tweets.values.tolist(), start='A1', end=None, dimension='ROWS', overwrite=False)\n",
    "   \n",
    "        #else:\n",
    "            #print('No new content to add')\n",
    "            \n",
    "    def get_staged_tweets(self):\n",
    "        return self.staged_tweets\n",
    "\n",
    "    \n",
    "ts = tweet_screener(stops, staged_tweets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_formatter:\n",
    "    def __init__(self):\n",
    "        self\n",
    "        # init the Google API translator\n",
    "        self.translator = Translator()\n",
    "    \n",
    "    def tweets_to_df(self,json):\n",
    "\n",
    "        \n",
    "        cols = ['create_date','create_time','tweet_url','tweet_id','author_username',\\\n",
    "                'reference_url','text','orig_lang','orig_text']\n",
    "        df_tweets = pd.DataFrame(columns=cols)\n",
    "        \n",
    "        if json['meta']['result_count'] == 0:\n",
    "            #print('no results')\n",
    "            return df_tweets # return an empty df\n",
    "        \n",
    "        \n",
    "        for tweet in json['data']:\n",
    "            \n",
    "            tweet_id = 'tweet_id:' + str(jmespath.search(\"id\", tweet))\n",
    "            #print(tweet_id)\n",
    "            urls = jmespath.search(\"entities.urls[*].expanded_url\", tweet)\n",
    "            primary_url = self.screen_urls(urls)\n",
    "            \n",
    "            ## if there's no primary url, don't bother with the rest\n",
    "            if primary_url == 'none found':\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            \n",
    "            text = jmespath.search(\"text\", tweet)\n",
    "            orig_text = text\n",
    "            text = self.scrub_text(text)\n",
    "            original_lang = jmespath.search(\"lang\", tweet)\n",
    "            if original_lang != 'en':\n",
    "                try:\n",
    "                    #print(f'translating from {original_lang} -- {text}')\n",
    "                    translation = self.translator.translate(text, src=original_lang, dest='en')\n",
    "                    text = translation.text\n",
    "                except:\n",
    "                    print(f'translation issue {original_lang}')\n",
    "            \n",
    "            created_date = jmespath.search(\"created_at\", tweet)\n",
    "            create_date, create_time = created_date.split('T')\n",
    "            create_time = create_time[:8]\n",
    "            \n",
    "            author_id = jmespath.search(\"author_id\", tweet)\n",
    "            author_username = self.get_author_info(author_id)\n",
    "            \n",
    "            tweet_url = f'https://twitter.com/{author_username}/status/{tweet_id}'\n",
    "            \n",
    "            tweet_info = [create_date, create_time, tweet_url, tweet_id, author_username, primary_url, text, original_lang, orig_text ]\n",
    "\n",
    "            df_tweets.loc[len(df_tweets)] = tweet_info\n",
    "            \n",
    "            df_tweets = df_tweets.drop_duplicates(subset='reference_url')\n",
    "            \n",
    "            \n",
    "        return df_tweets\n",
    "            \n",
    "    def scrub_text(self,t):\n",
    "    \n",
    "        if isinstance(t, str):\n",
    "            #remove newlines\n",
    "            t = re.sub('\\n', ' ', t)\n",
    "            #remove amp\n",
    "            t = re.sub('&amp;', ' and ', t)\n",
    "            #remove pipes because I use them as delimiters\n",
    "            t = t.replace('|', ' ')  \n",
    "            #remove unicode special chars\n",
    "            string_encode = t.encode(\"ascii\", \"ignore\")\n",
    "            t = string_encode.decode()\n",
    "\n",
    "        return t\n",
    "    \n",
    "    def screen_urls(self,urls):\n",
    "        #capture urls\n",
    "        patterns = [re.compile(\"pic\\.twitter\\.com\"), \n",
    "                    re.compile(\"twitter\\.com\"),\n",
    "                    re.compile(\"amzn\\.\"),\n",
    "                    re.compile(\"youtube\\.\"),\n",
    "                    re.compile(\"linkedin\\.\"),\n",
    "                   ]\n",
    "        returnUrls = []\n",
    "\n",
    "        for url in urls:\n",
    "            include = 'Y'\n",
    "            for pattern in patterns:\n",
    "                if pattern.search(url) != None:\n",
    "                    include = 'N'\n",
    "            if include == 'Y':\n",
    "                returnUrls.append(url)\n",
    "                \n",
    "        main_url = 'none found'      \n",
    "        if len(returnUrls) > 0:\n",
    "            main_url = returnUrls[0]\n",
    "\n",
    "        return main_url\n",
    "    \n",
    "    \n",
    "    def get_author_info(self,author_id):\n",
    "        global DF_AUTHORS\n",
    "        global runDate\n",
    "        #add the \"TID\"\n",
    "        internal_author_id = 'TID' + author_id\n",
    "\n",
    "        auth_list = DF_AUTHORS[DF_AUTHORS['author_id'] == internal_author_id].values.tolist()\n",
    "        if len(auth_list) == 1:\n",
    "            where = 'locally'\n",
    "            author_username = auth_list[0][3]\n",
    "\n",
    "        elif len(auth_list) > 1:\n",
    "            author_username = 'UNKN_MultipleResultsReturned'\n",
    "\n",
    "        else:\n",
    "            print(f'no individual author match on {author_id} ... trying twitter')\n",
    "\n",
    "            userFields = 'location,url,description,entities'\n",
    "            api_getAuthor = f'https://api.twitter.com/2/users?ids={author_id}&user.fields={userFields}'\n",
    "            response = requests.get(api_getAuthor, headers={\"Authorization\":bearer})\n",
    "            author_from_twitter = json.loads(response.text)\n",
    "            time.sleep(1) #throttle volume\n",
    "            #print(f' found {author_from_twitter}')\n",
    "\n",
    "            if 'data' in author_from_twitter:\n",
    "                where = 'twitter'\n",
    "\n",
    "                author_name = author_from_twitter['data'][0]['name']\n",
    "                author_username = author_from_twitter['data'][0]['username']\n",
    "                try:\n",
    "                    author_description = author_from_twitter['data'][0]['description']\n",
    "                except:\n",
    "                    author_description = ''\n",
    "                try:\n",
    "                    author_url = author_from_twitter['data'][0]['url']\n",
    "                except:\n",
    "                    author_url = ''\n",
    "\n",
    "            elif 'errors' in   author_from_twitter:\n",
    "                author_name = author_from_twitter['errors'][0]['title']\n",
    "                author_username = author_from_twitter['errors'][0]['title']\n",
    "                author_description = author_from_twitter['errors'][0]['detail']\n",
    "                author_url = author_from_twitter['errors'][0]['type']\n",
    "\n",
    "            else:\n",
    "                author_username = 'UNKNN_NoResultsReturned'\n",
    "\n",
    "\n",
    "\n",
    "            # add values to google sheets\n",
    "            vals= [runDate, internal_author_id, author_name, author_username, author_description, author_url]\n",
    "\n",
    "            #print(f'before cleanup of {vals}')\n",
    "            #clean up text \n",
    "            for i, item in enumerate(vals):\n",
    "                vals[i] = self.scrub_text(item)\n",
    "            #print(f'after cleanup of {vals}')\n",
    "\n",
    "            twitter_authors.append_table(vals, start='A1', end=None, dimension='ROWS', overwrite=False)\n",
    "\n",
    "            # add values to DF_AUTHORS\n",
    "            r = pd.Series(vals, index = DF_AUTHORS.columns)\n",
    "            DF_AUTHORS = DF_AUTHORS.append(r, ignore_index=True)\n",
    "\n",
    "\n",
    "        return author_username\n",
    "\n",
    "    \n",
    "\n",
    "#assert get_author_info('2576444334') == 'TheMissingMaps' #positive use-case\n",
    "#assert get_author_info('1477759062884421631') == 'Not Found Error' #negative use-case\n",
    "\n",
    "\n",
    "#assert scrub_text('asdfa|sdf &amp; dsdfgsdfg') == 'asdfa sdf  and  dsdfgsdfg'\n",
    "tf = tweet_formatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "no individual author match on 832277390302789634 ... trying twitter\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "scanning for humanitarian surge\n",
      "no individual author match on 26792275 ... trying twitter\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "scanning for predicting surge\n",
      "no individual author match on 2752025841 ... trying twitter\n",
      "translation issue zxx\n",
      "scanning for disaster assessment\n",
      "no individual author match on 1427211144091955202 ... trying twitter\n",
      "translation issue zxx\n",
      "no individual author match on 1105920842 ... trying twitter\n",
      "no individual author match on 66618330 ... trying twitter\n",
      "no individual author match on 29158151 ... trying twitter\n",
      "scanning for disaster blockchain\n",
      "no individual author match on 22744408 ... trying twitter\n",
      "translation issue qme\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 1115195820393938947 ... trying twitter\n",
      "no individual author match on 1595328493788831745 ... trying twitter\n",
      "no individual author match on 1200347068096753665 ... trying twitter\n",
      "no individual author match on 947177384620515328 ... trying twitter\n",
      "no individual author match on 2823365823 ... trying twitter\n",
      "translation issue zxx\n",
      "no individual author match on 1485360056526942212 ... trying twitter\n",
      "no individual author match on 1247254267867496448 ... trying twitter\n",
      "no individual author match on 75126870 ... trying twitter\n",
      "no individual author match on 74211791 ... trying twitter\n",
      "no individual author match on 895279846028439552 ... trying twitter\n",
      "no individual author match on 468793568 ... trying twitter\n",
      "no individual author match on 868487340729040896 ... trying twitter\n",
      "translation issue zxx\n",
      "no individual author match on 1520746521259565057 ... trying twitter\n",
      "no individual author match on 706470316873228288 ... trying twitter\n",
      "no individual author match on 811783063 ... trying twitter\n",
      "no individual author match on 1260703806536536064 ... trying twitter\n",
      "no individual author match on 192545209 ... trying twitter\n",
      "no individual author match on 1339977932626948097 ... trying twitter\n",
      "no individual author match on 256451488 ... trying twitter\n",
      "no individual author match on 1294480360131174401 ... trying twitter\n",
      "no individual author match on 1097964200817766401 ... trying twitter\n",
      "no individual author match on 1347047778342625281 ... trying twitter\n",
      "no individual author match on 1439009690332053505 ... trying twitter\n",
      "no individual author match on 1582610063339900933 ... trying twitter\n",
      "no individual author match on 1282034690833231873 ... trying twitter\n",
      "no individual author match on 24019986 ... trying twitter\n",
      "no individual author match on 1575045417087164416 ... trying twitter\n",
      "no individual author match on 83919249 ... trying twitter\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "translation issue zxx\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 1\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "scanning for predicting surge\n",
      "translation issue zxx\n",
      "scanning for disaster assessment\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "translation issue qme\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 879633388302475264 ... trying twitter\n",
      "no individual author match on 1382339794865246214 ... trying twitter\n",
      "no individual author match on 1319756178830823425 ... trying twitter\n",
      "no individual author match on 67949754 ... trying twitter\n",
      "no individual author match on 815482013532553216 ... trying twitter\n",
      "no individual author match on 1353027698 ... trying twitter\n",
      "no individual author match on 1091106767306661888 ... trying twitter\n",
      "no individual author match on 3060539594 ... trying twitter\n",
      "no individual author match on 314692357 ... trying twitter\n",
      "no individual author match on 705351186023628800 ... trying twitter\n",
      "no individual author match on 1088939389244194816 ... trying twitter\n",
      "no individual author match on 172081908 ... trying twitter\n",
      "no individual author match on 181055287 ... trying twitter\n",
      "no individual author match on 292945070 ... trying twitter\n",
      "no individual author match on 3323243956 ... trying twitter\n",
      "no individual author match on 42575294 ... trying twitter\n",
      "translation issue zxx\n",
      "translation issue zxx\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "translation issue zxx\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 2\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "no individual author match on 151431772 ... trying twitter\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "no individual author match on 32391088 ... trying twitter\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "no individual author match on 1471101606775119873 ... trying twitter\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "no individual author match on 1404653376 ... trying twitter\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "no individual author match on 299738875 ... trying twitter\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "scanning for predicting surge\n",
      "translation issue zxx\n",
      "scanning for disaster assessment\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 1577956336326828035 ... trying twitter\n",
      "no individual author match on 4480294462 ... trying twitter\n",
      "no individual author match on 280083025 ... trying twitter\n",
      "no individual author match on 1519834421939032064 ... trying twitter\n",
      "no individual author match on 2297072474 ... trying twitter\n",
      "no individual author match on 73748212 ... trying twitter\n",
      "no individual author match on 819731112004096000 ... trying twitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no individual author match on 919888112897155073 ... trying twitter\n",
      "no individual author match on 2998466318 ... trying twitter\n",
      "no individual author match on 1514639766 ... trying twitter\n",
      "no individual author match on 1562486133019525122 ... trying twitter\n",
      "no individual author match on 1460165219007844352 ... trying twitter\n",
      "no individual author match on 2591540467 ... trying twitter\n",
      "no individual author match on 14857413 ... trying twitter\n",
      "no individual author match on 26048864 ... trying twitter\n",
      "no individual author match on 1347879487363538944 ... trying twitter\n",
      "no individual author match on 1508429528338771970 ... trying twitter\n",
      "no individual author match on 532818331 ... trying twitter\n",
      "no individual author match on 2991516460 ... trying twitter\n",
      "no individual author match on 1715832776 ... trying twitter\n",
      "no individual author match on 110665708 ... trying twitter\n",
      "no individual author match on 1388165477810196484 ... trying twitter\n",
      "no individual author match on 198618575 ... trying twitter\n",
      "no individual author match on 241024381 ... trying twitter\n",
      "no individual author match on 64727768 ... trying twitter\n",
      "no individual author match on 1240935329462444033 ... trying twitter\n",
      "translation issue zxx\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "translation issue zxx\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 3\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "scanning for predicting surge\n",
      "translation issue zxx\n",
      "scanning for disaster assessment\n",
      "no individual author match on 283638910 ... trying twitter\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 147463326 ... trying twitter\n",
      "no individual author match on 1565503913742123008 ... trying twitter\n",
      "no individual author match on 1071152267322146817 ... trying twitter\n",
      "no individual author match on 1464301985218412554 ... trying twitter\n",
      "no individual author match on 499891600 ... trying twitter\n",
      "no individual author match on 1336249960405331971 ... trying twitter\n",
      "no individual author match on 271407243 ... trying twitter\n",
      "no individual author match on 2860651996 ... trying twitter\n",
      "no individual author match on 1314954566354391040 ... trying twitter\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 4\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "no individual author match on 596704112 ... trying twitter\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "no individual author match on 980787263549427713 ... trying twitter\n",
      "no individual author match on 2726511122 ... trying twitter\n",
      "scanning for predicting surge\n",
      "translation issue zxx\n",
      "scanning for disaster assessment\n",
      "no individual author match on 1406909301961244676 ... trying twitter\n",
      "translation issue in\n",
      "no individual author match on 21287066 ... trying twitter\n",
      "no individual author match on 1278732367176310795 ... trying twitter\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "translation issue in\n",
      "no individual author match on 1478265540330147840 ... trying twitter\n",
      "no individual author match on 130672343 ... trying twitter\n",
      "no individual author match on 429724798 ... trying twitter\n",
      "no individual author match on 435692689 ... trying twitter\n",
      "no individual author match on 101712819 ... trying twitter\n",
      "no individual author match on 98944200 ... trying twitter\n",
      "no individual author match on 856540348650319873 ... trying twitter\n",
      "no individual author match on 141937125 ... trying twitter\n",
      "no individual author match on 120649988 ... trying twitter\n",
      "no individual author match on 472815015 ... trying twitter\n",
      "no individual author match on 1581949412787593217 ... trying twitter\n",
      "no individual author match on 1417119766058135553 ... trying twitter\n",
      "no individual author match on 1587467104248365057 ... trying twitter\n",
      "no individual author match on 579419881 ... trying twitter\n",
      "no individual author match on 2484673278 ... trying twitter\n",
      "no individual author match on 1597374109993705477 ... trying twitter\n",
      "no individual author match on 56254987 ... trying twitter\n",
      "no individual author match on 303129981 ... trying twitter\n",
      "no individual author match on 912547491878379520 ... trying twitter\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 5\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "no individual author match on 267789059 ... trying twitter\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "no individual author match on 17650854 ... trying twitter\n",
      "no individual author match on 1968341269 ... trying twitter\n",
      "no individual author match on 377067103 ... trying twitter\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "no individual author match on 16204225 ... trying twitter\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "no individual author match on 1112732775419953158 ... trying twitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "no individual author match on 1363651327 ... trying twitter\n",
      "scanning for predicting surge\n",
      "scanning for disaster assessment\n",
      "translation issue in\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 22152077 ... trying twitter\n",
      "no individual author match on 120279447 ... trying twitter\n",
      "no individual author match on 1513979206409113602 ... trying twitter\n",
      "no individual author match on 322364182 ... trying twitter\n",
      "no individual author match on 88989964 ... trying twitter\n",
      "no individual author match on 15450996 ... trying twitter\n",
      "no individual author match on 738735873538293761 ... trying twitter\n",
      "no individual author match on 1519015333646016520 ... trying twitter\n",
      "no individual author match on 77916866 ... trying twitter\n",
      "no individual author match on 822844948433301504 ... trying twitter\n",
      "no individual author match on 233208927 ... trying twitter\n",
      "no individual author match on 777239667531927553 ... trying twitter\n",
      "no individual author match on 1137004829778370561 ... trying twitter\n",
      "no individual author match on 1297574551216414720 ... trying twitter\n",
      "no individual author match on 18195463 ... trying twitter\n",
      "no individual author match on 454197888 ... trying twitter\n",
      "no individual author match on 1110114036 ... trying twitter\n",
      "no individual author match on 1094566645924810753 ... trying twitter\n",
      "no individual author match on 1517431881037451265 ... trying twitter\n",
      "no individual author match on 828667794258526208 ... trying twitter\n",
      "no individual author match on 3167546573 ... trying twitter\n",
      "no individual author match on 931598362125664256 ... trying twitter\n",
      "no individual author match on 1506619836549505024 ... trying twitter\n",
      "no individual author match on 1006237459 ... trying twitter\n",
      "translation issue in\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 6\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "no individual author match on 1044721639001337859 ... trying twitter\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "no individual author match on 1184707213392302080 ... trying twitter\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "no individual author match on 19224439 ... trying twitter\n",
      "no individual author match on 126254190 ... trying twitter\n",
      "no individual author match on 25053299 ... trying twitter\n",
      "scanning for predicting surge\n",
      "scanning for disaster assessment\n",
      "translation issue in\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 1007554963558555648 ... trying twitter\n",
      "no individual author match on 1519408650028302336 ... trying twitter\n",
      "no individual author match on 197590339 ... trying twitter\n",
      "no individual author match on 882774873117835264 ... trying twitter\n",
      "no individual author match on 1389089402287050752 ... trying twitter\n",
      "no individual author match on 1063277316200108033 ... trying twitter\n",
      "translation issue in\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 7\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "no individual author match on 1027869370872025088 ... trying twitter\n",
      "scanning for blockchain refugee\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "no individual author match on 885117473179258882 ... trying twitter\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "no individual author match on 241223036 ... trying twitter\n",
      "no individual author match on 118773285 ... trying twitter\n",
      "no individual author match on 635258821 ... trying twitter\n",
      "no individual author match on 1510720961082970112 ... trying twitter\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "no individual author match on 2218248985 ... trying twitter\n",
      "scanning for predicting surge\n",
      "scanning for disaster assessment\n",
      "no individual author match on 2156108617 ... trying twitter\n",
      "translation issue in\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "no individual author match on 1550047742399246343 ... trying twitter\n",
      "no individual author match on 942937855126409218 ... trying twitter\n",
      "no individual author match on 1671166392 ... trying twitter\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "translation issue und\n",
      "no individual author match on 4783834469 ... trying twitter\n",
      "no individual author match on 9803552 ... trying twitter\n",
      "no individual author match on 1379041400076038151 ... trying twitter\n",
      "no individual author match on 134449438 ... trying twitter\n",
      "no individual author match on 1588630179081064449 ... trying twitter\n",
      "translation issue und\n",
      "no individual author match on 964033723 ... trying twitter\n",
      "no individual author match on 1412793002167357450 ... trying twitter\n",
      "translation issue und\n",
      "no individual author match on 979285160662269952 ... trying twitter\n",
      "no individual author match on 1408533060 ... trying twitter\n",
      "translation issue und\n",
      "no individual author match on 1510213039 ... trying twitter\n",
      "translation issue in\n",
      "translation issue und\n",
      "no individual author match on 3360734176 ... trying twitter\n",
      "no individual author match on 2350169149 ... trying twitter\n",
      "no individual author match on 822113708 ... trying twitter\n",
      "no individual author match on 1348039734296195074 ... trying twitter\n",
      "no individual author match on 26471523 ... trying twitter\n",
      "no individual author match on 385307075 ... trying twitter\n",
      "no individual author match on 1462769206345314308 ... trying twitter\n",
      "no individual author match on 889496054 ... trying twitter\n",
      "no individual author match on 1582119917566001152 ... trying twitter\n",
      "no individual author match on 1529293103323959296 ... trying twitter\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "scanning for Farrell_Diana\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 8\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "scanning for blockchain refugee\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "no individual author match on 83860254 ... trying twitter\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "scanning for predicting surge\n",
      "no individual author match on 1578858580337430529 ... trying twitter\n",
      "scanning for disaster assessment\n",
      "no individual author match on 262657733 ... trying twitter\n",
      "translation issue in\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "no individual author match on 50690488 ... trying twitter\n",
      "translation issue zxx\n",
      "no individual author match on 36132341 ... trying twitter\n",
      "no individual author match on 335932931 ... trying twitter\n",
      "no individual author match on 139146238 ... trying twitter\n",
      "no individual author match on 390357930 ... trying twitter\n",
      "no individual author match on 19811523 ... trying twitter\n",
      "no individual author match on 767122751110975488 ... trying twitter\n",
      "no individual author match on 390970453 ... trying twitter\n",
      "no individual author match on 1299818596827267072 ... trying twitter\n",
      "no individual author match on 57435738 ... trying twitter\n",
      "no individual author match on 869311775761498112 ... trying twitter\n",
      "no individual author match on 14723614 ... trying twitter\n",
      "no individual author match on 1396285295881072640 ... trying twitter\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 121026141 ... trying twitter\n",
      "no individual author match on 16568530 ... trying twitter\n",
      "no individual author match on 2758880940 ... trying twitter\n",
      "no individual author match on 1452800597720289282 ... trying twitter\n",
      "no individual author match on 1582617889928056832 ... trying twitter\n",
      "no individual author match on 44917831 ... trying twitter\n",
      "no individual author match on 1024584285620236288 ... trying twitter\n",
      "translation issue und\n",
      "translation issue und\n",
      "translation issue und\n",
      "translation issue und\n",
      "translation issue in\n",
      "translation issue und\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 9\n",
      "done\n",
      "scanning for humanitarian response blockchain\n",
      "scanning for humanitarian blockchain\n",
      "scanning for blockchain privacy risk\n",
      "no individual author match on 241797498 ... trying twitter\n",
      "scanning for blockchain refugee\n",
      "scanning for humanitarian response drones\n",
      "scanning for humanitarian drone\n",
      "scanning for drone damage assessment\n",
      "scanning for humanitarian drone mapping\n",
      "scanning for humanitarian geospatial\n",
      "scanning for humanitarian mapping\n",
      "scanning for humanitarian crowd map\n",
      "scanning for humanitarian data collection\n",
      "scanning for humanitarian analyze data\n",
      "scanning for pre-disaster data\n",
      "scanning for humanitarian assessment\n",
      "no individual author match on 1268997548573941761 ... trying twitter\n",
      "scanning for humanitarian surge\n",
      "scanning for surge assessment\n",
      "scanning for predicting humanitarian\n",
      "scanning for predicting crisis\n",
      "no individual author match on 1422917253532721158 ... trying twitter\n",
      "no individual author match on 20378642 ... trying twitter\n",
      "scanning for predicting surge\n",
      "scanning for disaster assessment\n",
      "no individual author match on 1071011177143173123 ... trying twitter\n",
      "translation issue in\n",
      "translation issue zxx\n",
      "scanning for disaster blockchain\n",
      "translation issue zxx\n",
      "no individual author match on 239792086 ... trying twitter\n",
      "no individual author match on 25520229 ... trying twitter\n",
      "no individual author match on 1054578391465320448 ... trying twitter\n",
      "no individual author match on 163175255 ... trying twitter\n",
      "no individual author match on 1309600377931784193 ... trying twitter\n",
      "no individual author match on 1463258756591112194 ... trying twitter\n",
      "no individual author match on 45836478 ... trying twitter\n",
      "no individual author match on 321895112 ... trying twitter\n",
      "translation issue zxx\n",
      "no individual author match on 420490529 ... trying twitter\n",
      "no individual author match on 1595143520460980302 ... trying twitter\n",
      "no individual author match on 215368262 ... trying twitter\n",
      "translation issue zxx\n",
      "no individual author match on 3278980016 ... trying twitter\n",
      "no individual author match on 1116160286979969025 ... trying twitter\n",
      "translation issue qme\n",
      "no individual author match on 223279139 ... trying twitter\n",
      "translation issue zxx\n",
      "translation issue qme\n",
      "scanning for disaster response\n",
      "no individual author match on 1382631436906332160 ... trying twitter\n",
      "no individual author match on 844359017732292609 ... trying twitter\n",
      "no individual author match on 1244244756353429505 ... trying twitter\n",
      "no individual author match on 3260870630 ... trying twitter\n",
      "translation issue und\n",
      "no individual author match on 297445799 ... trying twitter\n",
      "no individual author match on 1632821258 ... trying twitter\n",
      "no individual author match on 403715213 ... trying twitter\n",
      "no individual author match on 841854793240911872 ... trying twitter\n",
      "no individual author match on 1902877628 ... trying twitter\n",
      "no individual author match on 221057287 ... trying twitter\n",
      "no individual author match on 3398895616 ... trying twitter\n",
      "no individual author match on 22933218 ... trying twitter\n",
      "no individual author match on 3439953258 ... trying twitter\n",
      "no individual author match on 307716815 ... trying twitter\n",
      "translation issue und\n",
      "translation issue und\n",
      "translation issue und\n",
      "translation issue und\n",
      "translation issue in\n",
      "translation issue und\n",
      "scanning for disaster openstreetmap\n",
      "scanning for PatrickMeier\n",
      "scanning for ivangayton\n",
      "scanning for HIPNorway\n",
      "scanning for UN_Innovation\n",
      "scanning for ICRC_Innovation\n",
      "scanning for ricap_undp\n",
      "scanning for Catalyst_2030\n",
      "scanning for solvozplatform\n",
      "scanning for CEBaP_evidence\n",
      "scanning for feraldata\n",
      "scanning for Caterina\n",
      "scanning for Farrell_Diana\n",
      "scanning for Mgorbis\n",
      "scanning for AndreaKerzner\n",
      "scanning for alicekorngold\n",
      "scanning for susanmcp1\n",
      "scanning for nilofer\n",
      "scanning for ZainabSalbi\n",
      "scanning for wayan_vota\n",
      "scanning for WeRobotics\n",
      "scanning for MykolaKozyr\n",
      "scanning for ECAAS_AGData\n",
      "scanning for kobotoolbox\n",
      "scanning for IvanaJurko\n",
      "about to sleep for 36000, i is 10\n"
     ]
    }
   ],
   "source": [
    "tscanner.scan_all_active_terms(stop_after,sleeptime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
