{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search for specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheets Variables\n",
    "# variables that shouldn't change by person\n",
    "\n",
    "SEARCH_WORKBOOK = {'sheet_id' : '1QjvZOnkCJM-BcRvMlP0XsN0hJeanKPiK7_e37uJiKek',\n",
    "                    'range_names': ['twitter_keywords','stop_phrases']}\n",
    "\n",
    "RESULTS_WORKBOOK = {'sheet_id' : '1_6-O1D7UtbA4PiDNr71Fm-kA_OJk8S7RBctkGjZRIVk',\n",
    "                     'range_names': ['tweet_list','staged_tweets','twitter_authors']}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "#import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "\n",
    "#https://www.thepythoncode.com/article/translate-text-in-python\n",
    "from googletrans import Translator, constants\n",
    "from pprint import pprint\n",
    "\n",
    "#to connect to google\n",
    "from google.oauth2 import service_account\n",
    "import pygsheets\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "import jmespath\n",
    "import urlexpander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User-Specific Variables\n",
    "\n",
    "#Source of Inputs: GoogleSheets or LocalFile\n",
    "control_input = 'googlesheets'\n",
    "\n",
    "\n",
    "#for a continuous loop, put 0 or a negative number.\n",
    "stop_after=-1\n",
    "sleeptime = 60*60*10 #10 hour sleep time... to gradually roll to different times of day to catch different time zones.\n",
    "search_languages = ['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "#twitter variables\n",
    "API_Key = os.environ.get(\"API_KEY\")\n",
    "API_Key_Secret = os.environ.get(\"API_KEY_SECRET\")\n",
    "Bearer_Token = os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_Key, API_Key_Secret)\n",
    "bearer = 'bearer ' + Bearer_Token\n",
    "\n",
    "#google variables\n",
    "SERVICE_ACCOUNT_FILE = os.environ.get(\"GOOGLE_SERVICE_ACCOUNT_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the Google API translator\n",
    "translator = Translator()\n",
    "\n",
    "runDate = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetQueryBuilder(searchTerms):\n",
    "    base_api_url = 'https://api.twitter.com/2/tweets/search/recent?query='\n",
    "    fixed_args = '-is:retweet  has:links '\n",
    "    \n",
    "    query_arg_prefix = fixed_args + searchTerms\n",
    "    query_arg_prefix = urllib.parse.quote(query_arg_prefix)\n",
    "    query_arg_suffix = '&max_results=100&tweet.fields=attachments,author_id,context_annotations,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,source,text,withheld&expansions=referenced_tweets.id,geo.place_id'\n",
    "    \n",
    "    query = base_api_url + query_arg_prefix + query_arg_suffix\n",
    "    \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_google_sheets_connector():\n",
    "    # Set up Google Credentials\n",
    "    #SERVICE_ACCOUNT_FILE = google_service_account_file\n",
    "\n",
    "    SCOPES = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    # connect\n",
    "    gc = pygsheets.authorize(credentials=service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, \n",
    "        scopes=SCOPES))\n",
    "    \n",
    "    return gc\n",
    "\n",
    "gc = instantiate_google_sheets_connector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load to workbook objects\n",
    "search_workbook = gc.open_by_key(SEARCH_WORKBOOK['sheet_id'])\n",
    "results_workbook = gc.open_by_key(RESULTS_WORKBOOK['sheet_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate read-only dataframes\n",
    "keywords = search_workbook[0]\n",
    "#df_r_keywords = pd.DataFrame(keywords.get_all_records())\n",
    "\n",
    "stop_phrases = search_workbook[1]\n",
    "#df_r_stop_phrases = pd.DataFrame(stop_phrases.get_all_records())\n",
    "\n",
    "#generate updatable dataframes\n",
    "tweet_list = results_workbook[0]\n",
    "#df_u_tweet_list = pd.DataFrame(tweet_list.get_all_records())\n",
    "\n",
    "staged_tweets = results_workbook[1]\n",
    "#df_u_staged_tweets = pd.DataFrame(staged_tweets.get_all_records())\n",
    "\n",
    "twitter_authors = results_workbook[2]\n",
    "global DF_AUTHORS\n",
    "DF_AUTHORS = pd.DataFrame(twitter_authors.get_all_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_queries(control_input):\n",
    "    \n",
    "    if control_input == 'googlesheets':\n",
    "        #google:\n",
    "\n",
    "        # Set up Google Credentials\n",
    "\n",
    "        SCOPES = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "        credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "        # connect\n",
    "        gc = pygsheets.authorize(credentials=service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES))\n",
    "        #sh = gc.open_by_link('https://docs.google.com/spreadsheets/d/1QjvZOnkCJM-BcRvMlP0XsN0hJeanKPiK7_e37uJiKek/edit?usp=sharing')\n",
    "        workbook = gc.open_by_key(SEARCH_WORKBOOK['sheet_id'])\n",
    "\n",
    "        keyword_worksheet = workbook[0]\n",
    "        df_keywords = pd.DataFrame(keyword_worksheet.get_all_records())\n",
    "        queries = df_keywords['key_words'][(df_keywords['record_type'] == 'search_term') &\\\n",
    "                                           (df_keywords['status'] == 'Active')].values.tolist()\n",
    "        #print(df_keywords['key_words'][(df_keywords['record_type'] == 'search_term') \\\n",
    "        #                                   (df_keywords['status'])])\n",
    "\n",
    "    else: #legacy or testing\n",
    "        queries = [\n",
    "\n",
    "            'disaster assessment',\n",
    "            'disaster blockchain',\n",
    "            'disaster response',\n",
    "            'disaster openstreetmap',\n",
    "            'openstreetmap'\n",
    "\n",
    "        ]\n",
    "    return queries\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_google_sheets(parsed):\n",
    "    t_keys = []\n",
    "    t_details = []\n",
    "    \n",
    "    nourlcnt=0\n",
    "    dupcnt=0\n",
    "    newcnt=0\n",
    "    urldupcnt=0\n",
    "\n",
    "    #df_already_saved_tweets = pd.DataFrame(tweet_list.get_all_records()) #seed the list with one row of data\n",
    "    df_already_saved_tweets = pd.DataFrame(staged_tweets.get_all_records()) #seed the list with one row of data\n",
    "\n",
    "    for tweet in parsed['data']:\n",
    "        urls = jmespath.search(\"entities.urls[*].expanded_url\", tweet)\n",
    "        urls = remove_twitter_urls(urls)\n",
    "        #print(f'urls are {urls}')\n",
    "        if len(urls) == 0:\n",
    "            nourlcnt +=1\n",
    "            next\n",
    "\n",
    "        tweet_id = str(jmespath.search(\"id\", tweet))\n",
    "\n",
    "        #determine the number of times that particular tweet has already been saved\n",
    "        saved_cnt = df_already_saved_tweets[df_already_saved_tweets['tweet_id'].astype(str) == tweet_id].shape[0]\n",
    "        \n",
    "                                               \n",
    "        if saved_cnt > 0:\n",
    "            #hard dedupe\n",
    "            dupcnt+=1\n",
    "            next\n",
    "\n",
    "        else:\n",
    "        #next dedupes will return a dupe-code instead\n",
    "            tweet_info = extract_tweet_info(tweet)\n",
    "            if tweet_info[5] == 'NA': #if all urls turn out to be twitter urls, skip\n",
    "                nourlcnt+=1\n",
    "                next\n",
    "            else:\n",
    "                url_dupe_cnt = df_already_saved_tweets[df_already_saved_tweets['reference_url'] == tweet_info[5]].shape[0]\n",
    "                if url_dupe_cnt > 0:\n",
    "                    urldupcnt+=1\n",
    "                    print(f'{tweet_id} dupeurl {tweet_info[5]}')\n",
    "                    next\n",
    "                else:\n",
    "                    t_details.append(tweet_info)\n",
    "                    t_keys.append([runDate,tweet_id, 'query_twitter script'])\n",
    "                    newcnt+=1\n",
    "\n",
    "    #if anything remains, write to the tweet_list sheet\n",
    "    if len(t_keys) > 0:\n",
    "        tweet_list.append_table(t_keys, start='A1', end=None, dimension='ROWS', overwrite=False)\n",
    "        staged_tweets.append_table(t_details, start='A1', end=None, dimension='ROWS', overwrite=False)\n",
    "    else:\n",
    "        print('No new content to add')\n",
    "\n",
    "    print(f'AT completion of write to google sheets, stats are {dupcnt} tweet id dupes, {nourlcnt} no urls, \\\n",
    "           {urldupcnt} url dupes, {newcnt} new')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_urls(urls):\n",
    "    \n",
    "    #capture urls\n",
    "    patterns = [re.compile(\"pic\\.twitter\\.com\"), \n",
    "                re.compile(\"twitter\\.com\")]\n",
    "    returnUrl = []\n",
    "    \n",
    "    for url in urls:\n",
    "        include = 'Y'\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(url) != None:\n",
    "                include = 'N'\n",
    "        if include == 'Y':\n",
    "            returnUrl.append(url)\n",
    "    \n",
    "    return returnUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_info(author_id):\n",
    "    global DF_AUTHORS\n",
    "    global runDate\n",
    "    #add the \"TID\"\n",
    "    internal_author_id = 'TID' + author_id\n",
    "    \n",
    "    auth_list = DF_AUTHORS[DF_AUTHORS['author_id'] == internal_author_id].values.tolist()\n",
    "    if len(auth_list) == 1:\n",
    "        where = 'locally'\n",
    "        author_username = auth_list[0][3]\n",
    "\n",
    "    elif len(auth_list) > 1:\n",
    "        author_username = 'UNKN_MultipleResultsReturned'\n",
    "\n",
    "    else:\n",
    "        print(f'no individual author match on {author_id} ... trying twitter')\n",
    "        \n",
    "        userFields = 'location,url,description,entities'\n",
    "        api_getAuthor = f'https://api.twitter.com/2/users?ids={author_id}&user.fields={userFields}'\n",
    "        response = requests.get(api_getAuthor, headers={\"Authorization\":bearer})\n",
    "        author_from_twitter = json.loads(response.text)\n",
    "        time.sleep(2) #throttle volume\n",
    "        #print(f' found {author_from_twitter}')\n",
    "\n",
    "        if 'data' in author_from_twitter:\n",
    "            where = 'twitter'\n",
    "\n",
    "            author_name = author_from_twitter['data'][0]['name']\n",
    "            author_username = author_from_twitter['data'][0]['username']\n",
    "            try:\n",
    "                author_description = author_from_twitter['data'][0]['description']\n",
    "            except:\n",
    "                author_description = ''\n",
    "            try:\n",
    "                author_url = author_from_twitter['data'][0]['url']\n",
    "            except:\n",
    "                author_url = ''\n",
    "\n",
    "        elif 'errors' in   author_from_twitter:\n",
    "            author_name = author_from_twitter['errors'][0]['title']\n",
    "            author_username = author_from_twitter['errors'][0]['title']\n",
    "            author_description = author_from_twitter['errors'][0]['detail']\n",
    "            author_url = author_from_twitter['errors'][0]['type']\n",
    "\n",
    "        else:\n",
    "            author_username = 'UNKNN_NoResultsReturned'\n",
    "\n",
    "\n",
    "\n",
    "        # add values to google sheets\n",
    "        vals= [runDate, internal_author_id, author_name, author_username, author_description, author_url]\n",
    "        \n",
    "        #print(f'before cleanup of {vals}')\n",
    "        #clean up text \n",
    "        for i, item in enumerate(vals):\n",
    "            vals[i] = scrub_text(item)\n",
    "        #print(f'after cleanup of {vals}')\n",
    "            \n",
    "        twitter_authors.append_table(vals, start='A1', end=None, dimension='ROWS', overwrite=False)\n",
    "        \n",
    "        # add values to DF_AUTHORS\n",
    "        r = pd.Series(vals, index = DF_AUTHORS.columns)\n",
    "        DF_AUTHORS = DF_AUTHORS.append(r, ignore_index=True)\n",
    "        \n",
    "        \n",
    "    return author_username\n",
    "    #return [internal_author_id, author_name, author_username, author_description, author_url]\n",
    "    \n",
    "\n",
    "assert get_author_info('2576444334') == 'TheMissingMaps' #positive use-case\n",
    "assert get_author_info('1477759062884421631') == 'Not Found Error' #negative use-case\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cascade Dupe Checking\n",
    "1. Hard Dupe on TweetId\n",
    "2. Soft Dupe on similar tweet text\n",
    "3. Soft Dupe on target url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_info(tweet):\n",
    "    \n",
    "    tweet_id = str(jmespath.search(\"id\", tweet))\n",
    "    \n",
    "    urls = jmespath.search(\"entities.urls[*].expanded_url\", tweet)\n",
    "    urls = remove_twitter_urls(urls)\n",
    "            \n",
    "    text = jmespath.search(\"text\", tweet)\n",
    "    text = scrub_text(text)\n",
    "    \n",
    "\n",
    "    original_lang = jmespath.search(\"lang\", tweet)\n",
    "    if original_lang == 'en':\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            translation = translator.translate(text, src=original_lang, dest='en')\n",
    "            text = translation.text\n",
    "        except:\n",
    "            print(f'language issue {original_lang}')\n",
    "    \n",
    "\n",
    "    created_date = jmespath.search(\"created_at\", tweet)\n",
    "    create_date, create_time = created_date.split('T')\n",
    "    create_time = create_time[:8]\n",
    "    \n",
    "    author_id = jmespath.search(\"author_id\", tweet)\n",
    "    \n",
    "    \n",
    "    \n",
    "    author_username = get_author_info(author_id)\n",
    "    \n",
    "    tweet_url = f'https://twitter.com/{author_username}/status/{tweet_id}'\n",
    "    #print(f'urls are {urls}')\n",
    "    if len(urls) == 0:\n",
    "        urls.append('NA')\n",
    "    tweet_info = [create_date, create_time, tweet_url, tweet_id, author_username, urls[0], text ]\n",
    "    \n",
    "    return tweet_info\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_text(t):\n",
    "    \n",
    "    if isinstance(t, str):\n",
    "\n",
    "        #remove newlines\n",
    "        t = re.sub('\\n', ' ', t)\n",
    "\n",
    "        #remove amp\n",
    "        t = re.sub('&amp;', ' and ', t)\n",
    "\n",
    "          #remove pipes because I use them as delimiters\n",
    "        t = t.replace('|', ' ')  \n",
    "\n",
    "        #remove unicode special chars\n",
    "        string_encode = t.encode(\"ascii\", \"ignore\")\n",
    "        t = string_encode.decode()\n",
    "    \n",
    "    return t\n",
    "\n",
    "assert scrub_text('asdfa|sdf &amp; dsdfgsdfg') == 'asdfa sdf  and  dsdfgsdfg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "humanitarian response blockchain en yielded 0 results.\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian response blockchain_en.json | Record Count | 0\n",
      "\n",
      "\n",
      "\n",
      "humanitarian blockchain en yielded 7 results.\n",
      "attempting to write 7 records to google sheets.\n",
      "dupeurl https://odessa-journal.com/an-official-website-has-been-launched-to-collect-cryptocurrency-in-support-of-ukraine/\n",
      "dupeurl https://buff.ly/3WLROFd\n",
      "dupeurl http://MintMachine.Art\n",
      "dupeurl https://www.binance.charity/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 6 no urls,            4 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian blockchain_en.json | Record Count | 7\n",
      "\n",
      "\n",
      "\n",
      "blockchain privacy risk en yielded 3 results.\n",
      "attempting to write 3 records to google sheets.\n",
      "dupeurl https://cryptoandcoin.news/ftx-blocks-aztec-network-privacy-dapp-calling-it-a-high-risk-mixer/?feed_id=37910&_unique_id=636b72f2a0dda\n",
      "dupeurl https://opsmtrs.com/36dWI8q\n",
      "dupeurl https://opsmtrs.com/36dWI8q\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 0 no urls,            3 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_blockchain privacy risk_en.json | Record Count | 3\n",
      "\n",
      "\n",
      "\n",
      "blockchain refugee en yielded 1 results.\n",
      "attempting to write 1 records to google sheets.\n",
      "dupeurl https://www.unicc.org/news/2022/11/02/unicc-and-unhcr-at-the-stellar-meridian-2022-blockchain-conference/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 0 no urls,            1 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_blockchain refugee_en.json | Record Count | 1\n",
      "\n",
      "\n",
      "\n",
      "humanitarian response drones en yielded 0 results.\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian response drones_en.json | Record Count | 0\n",
      "\n",
      "\n",
      "\n",
      "humanitarian drone en yielded 24 results.\n",
      "attempting to write 24 records to google sheets.\n",
      "dupeurl https://www.rfi.fr/en/international-news/20221111-ethiopia-govt-says-aid-flowing-to-tigray-but-rebels-deny\n",
      "dupeurl https://www.usatoday.com/story/opinion/columnist/2022/03/10/biden-sanctions-afghanistan-humanitarian-crisis/6918023001/\n",
      "dupeurl https://youtu.be/TlO2gcs1YvM\n",
      "dupeurl http://indiecup.net/ukraine/donate\n",
      "dupeurl https://europa.eu/!BnhWm7\n",
      "dupeurl https://www.bild.de/video/clip/news-ausland/von-drohne-beschossen-russen-soldat-wirft-granaten-in-letzter-sekunde-weg-81864662.bild.html\n",
      "dupeurl https://www.cfr.org/blog/obamas-final-drone-strike-data\n",
      "dupeurl https://volatusllc.com/cag/\n",
      "dupeurl http://dlvr.it/ScQzWK\n",
      "dupeurl https://paxforpeace.nl/who-we-are/vacancies/paid-positions/senior-project-officer-humanitarian-disarmament\n",
      "dupeurl https://cynthiachung.substack.com/p/how-the-ukrainian-nationalist-movement-f4f\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 26 no urls,            11 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian drone_en.json | Record Count | 24\n",
      "\n",
      "\n",
      "\n",
      "drone damage assessment en yielded 1 results.\n",
      "attempting to write 1 records to google sheets.\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 2 no urls,            0 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_drone damage assessment_en.json | Record Count | 1\n",
      "\n",
      "\n",
      "\n",
      "humanitarian drone mapping en yielded 1 results.\n",
      "attempting to write 1 records to google sheets.\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 2 no urls,            0 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian drone mapping_en.json | Record Count | 1\n",
      "\n",
      "\n",
      "\n",
      "humanitarian geospatial en yielded 3 results.\n",
      "attempting to write 3 records to google sheets.\n",
      "dupeurl https://t.her.is/3TvoTSO\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 4 no urls,            1 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian geospatial_en.json | Record Count | 3\n",
      "\n",
      "\n",
      "\n",
      "humanitarian mapping en yielded 11 results.\n",
      "attempting to write 11 records to google sheets.\n",
      "dupeurl https://tanzdevtrust.org/crowd2map/\n",
      "dupeurl https://www.npr.org/sections/parallels/2016/10/02/495795717/when-disaster-strikes-he-creates-a-crisis-map-that-helps-save-lives\n",
      "dupeurl https://framaforms.org/webinar-using-qgis-in-the-humanitarian-and-development-sectors-1666972264\n",
      "dupeurl https://m.facebook.com/story.php?story_fbid=pfbid02hVAs1zCJmTgis7yfoxygA1t1giKjQpoZVuW8xhGASPKHe2iG8DbMmp2QZnR5PCeEl&id=100086008814890&sfnsn=mo\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 14 no urls,            4 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian mapping_en.json | Record Count | 11\n",
      "\n",
      "\n",
      "\n",
      "humanitarian crowd map en yielded 0 results.\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian crowd map_en.json | Record Count | 0\n",
      "\n",
      "\n",
      "\n",
      "humanitarian data collection en yielded 8 results.\n",
      "attempting to write 8 records to google sheets.\n",
      "dupeurl https://or-f.org/112780\n",
      "dupeurl https://or-f.org/112780\n",
      "dupeurl https://or-f.org/112780\n",
      "dupeurl https://or-f.org/112780\n",
      "dupeurl https://or-f.org/112780\n",
      "dupeurl https://or-f.org/112780\n",
      "dupeurl https://or-f.org/112780\n",
      "dupeurl https://or-f.org/112780\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 0 no urls,            8 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian data collection_en.json | Record Count | 8\n",
      "\n",
      "\n",
      "\n",
      "humanitarian analyze data en yielded 0 results.\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian analyze data_en.json | Record Count | 0\n",
      "\n",
      "\n",
      "\n",
      "pre-disaster data en yielded 0 results.\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_pre-disaster data_en.json | Record Count | 0\n",
      "\n",
      "\n",
      "\n",
      "humanitarian assessment en yielded 24 results.\n",
      "attempting to write 24 records to google sheets.\n",
      "dupeurl https://www.ids.ac.uk/opinions/marking-the-second-year-of-the-tigray-war/@UN\n",
      "dupeurl https://phantomtutors.com/questions/solved-assessment-for-the-week-point-of-distribution-layout-due-this-should-be-a-1-2-slide-ppt-layout-try-to-design-a-template-on-how-you-would-layouta-pod-to-support-citizens-during-a-disaster/?feed_id=583707&_unique_id=636fcea9968fd\n",
      "dupeurl https://www.ids.ac.uk/opinions/marking-the-second-year-of-the-tigray-war/@UN\n",
      "dupeurl https://www.ids.ac.uk/opinions/marking-the-second-year-of-the-tigray-war/@UN\n",
      "dupeurl https://www.ids.ac.uk/opinions/marking-the-second-year-of-the-tigray-war/@UN\n",
      "dupeurl https://www.ids.ac.uk/opinions/marking-the-second-year-of-the-tigray-war/@UN\n",
      "dupeurl https://www.ids.ac.uk/opinions/marking-the-second-year-of-the-tigray-war/@UN\n",
      "dupeurl https://www.ids.ac.uk/opinions/marking-the-second-year-of-the-tigray-war/@UN\n",
      "dupeurl https://bit.ly/AFG_WoA22\n",
      "dupeurl https://www.lightcastlebd.com/news/lightcastle-signs-contract-with-start-network-to-provide-due-diligence-support-for-strengthening-humanitarian-assistance-in-bangladesh/\n",
      "dupeurl https://www.epc.eu/en/publications/Refugee-protection-in-the-EU-Building-resilience-to-geopolitical-conf~4b4f4c\n",
      "dupeurl https://swn.af/en/index.php/2022/11/07/un-to-assess-humanitarian-needs-across-afghanistan/\n",
      "dupeurl https://swn.af/ocha-a-nationwide-assessment-of-humanitarian-needs-is-being-conducted-in-afghanistan/\n",
      "dupeurl https://8am.media/eng/assessment-for-humanitarian-needs-to-be-conducted-across-afghanistan-ocha/\n",
      "dupeurl https://8am.media/ocha-a-nationwide-assessment-of-humanitarian-needs-is-being-conducted-in-afghanistan/\n",
      "dupeurl https://ift.tt/KWNycVb\n",
      "dupeurl https://reliefweb.int/report/poland/poland-multi-sector-needs-assessment-protection-cash-market-humanitarian-assistance-october-2022\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 14 no urls,            17 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian assessment_en.json | Record Count | 24\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humanitarian surge en yielded 4 results.\n",
      "attempting to write 4 records to google sheets.\n",
      "dupeurl https://on.cfr.org/3G5x1GJ\n",
      "dupeurl https://www.euractiv.com/section/europe-s-east/news/migrants-rescued-from-swamp-on-polish-belarus-border-as-numbers-rise/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 4 no urls,            2 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_humanitarian surge_en.json | Record Count | 4\n",
      "\n",
      "\n",
      "\n",
      "surge assessment en yielded 12 results.\n",
      "attempting to write 12 records to google sheets.\n",
      "dupeurl http://bit.ly/3hwjm11\n",
      "dupeurl https://www.news4jax.com/news/local/2022/11/10/clay-county-significant-flooding-around-black-creek-doctors-inlet-due-to-storm-surge/?utm_source=twitter&utm_medium=social&utm_campaign=snd&utm_content=wjxt4\n",
      "dupeurl http://dlvr.it/ScYZZq\n",
      "dupeurl http://Nature.com\n",
      "dupeurl http://Nature.com\n",
      "dupeurl http://bit.ly/3DRqBbp\n",
      "dupeurl https://cera.coastalrisk.live/\n",
      "dupeurl https://deloi.tt/3taZXFN\n",
      "dupeurl https://cera.coastalrisk.live/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 6 no urls,            9 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_surge assessment_en.json | Record Count | 12\n",
      "\n",
      "\n",
      "\n",
      "predicting humanitarian en yielded 0 results.\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_predicting humanitarian_en.json | Record Count | 0\n",
      "\n",
      "\n",
      "\n",
      "predicting crisis en yielded 17 results.\n",
      "attempting to write 17 records to google sheets.\n",
      "dupeurl https://bit.ly/3DDnLI4\n",
      "dupeurl https://www.thecipherbrief.com/the-new-age-of-energy-crisis-and-diplomacy?utm_source=Join+the+Community+Subscribers&utm_campaign=75cb555469-EMAIL_CAMPAIGN_2022_11_10_02_57&utm_medium=email&utm_term=0_02cbee778d-75cb555469-122835010&mc_cid=75cb555469&mc_eid=232e99202c\n",
      "dupeurl http://www.socialist.net/doctor-doom-predicts-dark-days-for-capitalism.htm\n",
      "dupeurl https://www.dailysignal.com/2022/11/10/chinas-growing-naval-air-operations-in-east-asia-represent-dangerous-new-normal/\n",
      "dupeurl https://gulfnews.com/opinion/op-eds/cop27-climate-crisis-threatens-global-health-1.91797058#\n",
      "dupeurl https://www.bostonglobe.com/2022/11/07/business/us-northeast-is-hurtling-toward-winter-heating-crisis/\n",
      "dupeurl https://hubs.la/Q01rN6T00\n",
      "dupeurl https://www.aeaweb.org/articles?id=10.1257/jep.32.3.113\n",
      "dupeurl https://www.ghstandard.com/debt-crisis-looming-in-2023-un-warns-ghana-and-other-african-countries/\n",
      "dupeurl https://tradersblog.semwealth.com/mmm3-43-2/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 14 no urls,            10 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_predicting crisis_en.json | Record Count | 17\n",
      "\n",
      "\n",
      "\n",
      "predicting surge en yielded 13 results.\n",
      "attempting to write 13 records to google sheets.\n",
      "dupeurl https://www.npr.org/sections/health-shots/2022/11/11/1136039817/new-omicron-subvariants-now-dominant-in-the-u-s-raising-fears-of-a-winter-surge\n",
      "dupeurl https://www.businessinsider.com/mike-lindell-predicts-surge-in-republican-votes-at-midterms-2022-10?utm_source=ReviveOldPost&utm_medium=social&utm_campaign=ReviveOldPost\n",
      "dupeurl https://www.theinformation.com/articles/larry-summers-compares-the-surge-of-pain-in-tech-to-the-dot-com-bubble\n",
      "dupeurl https://anchor.fm/trista-di-genova/episodes/LegalAF-Meidastouch-Pollster-predicting-win-for-democrats--wblue-surge-in-turnout-e1qf5u2\n",
      "dupeurl https://www.wndu.com/2022/11/08/election-2022-gop-predicting-wins-democrats-brace-setbacks/?utm_source=twitter&utm_medium=social&utm_campaign=snd&utm_content=wndu\n",
      "dupeurl https://www.palmbeachpost.com/story/weather/2022/11/06/hurricane-center-predicting-tropical-cyclone-impact-florida/8287887001/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 14 no urls,            6 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_predicting surge_en.json | Record Count | 13\n",
      "\n",
      "\n",
      "\n",
      "disaster assessment en yielded 75 results.\n",
      "attempting to write 75 records to google sheets.\n",
      "dupeurl http://www.amazon.com/dp/B0BKWXHHNF?tag=it_books_com-20\n",
      "dupeurl https://www.nytimes.com/2022/11/08/climate/national-climate-assessment.html\n",
      "dupeurl https://www.rappler.com/life-and-style/travel/baguio-igorot-stone-kingdom-shut-down-permit-safety-issues/?utm_medium=Social&utm_source=Twitter#Echobox=1668304877-2\n",
      "dupeurl https://solution-content.amp.vg/web/c7qn2wphz3s2a\n",
      "dupeurl https://phantomtutors.com/questions/solved-assessment-for-the-week-point-of-distribution-layout-due-this-should-be-a-1-2-slide-ppt-layout-try-to-design-a-template-on-how-you-would-layouta-pod-to-support-citizens-during-a-disaster/?feed_id=583707&_unique_id=636fcea9968fd\n",
      "dupeurl https://www.rappler.com/life-and-style/travel/baguio-igorot-stone-kingdom-shut-down-permit-safety-issues/?utm_medium=Social&utm_source=Twitter#Echobox=1668216348-2\n",
      "dupeurl http://bit.ly/3hwjm11\n",
      "dupeurl https://bit.ly/3TmQ0j6\n",
      "dupeurl https://www.myjournals.org/?nr=713669\n",
      "dupeurl https://journals.lww.com/joem/Fulltext/2022/11000/COVID_19_Traumatic_Disaster_Appraisal_and_Stress.6.aspx\n",
      "dupeurl https://ournews.bs/preparing-for-a-disaster/\n",
      "dupeurl https://nowgrenada.com/2022/11/nadma-kicks-off-national-disaster-preparedness-baseline-assessment-project/\n",
      "dupeurl https://www.mikeylive.com/2022/11/10/nadma-hosts-kickoff-meeting-for-the-national-disaster-preparedness-baseline-assessment-project-with-stakeholders/\n",
      "dupeurl https://bit.ly/3FUMJof\n",
      "dupeurl https://bit.ly/3hzybAh\n",
      "dupeurl https://www.nytimes.com/2022/11/08/climate/national-climate-assessment.html?smid=tw-share\n",
      "dupeurl http://princeedwardisland.ca/en/service/provincial-disaster-financial-assistance-program\n",
      "dupeurl http://www.msn.com/en-ph/news/national/makati-rated-beyond-compliant-in-gawad-kalasag-disaster-risk-reduction-assessment/ar-AA13VA0M?ocid=ob-tw-enph-501\n",
      "dupeurl https://usanews.ltd/the-crown-season-5-review-debickis-diana-reigns-supreme-amid-a-monarchy-in-crisis/\n",
      "dupeurl https://newscenter.store/the-crown-season-5-assessment-debickis-diana-reigns-supreme-amid-a-monarchy-in-disaster/\n",
      "dupeurl https://bit.ly/3Ss94vS\n",
      "dupeurl https://www.rappler.com/life-and-style/travel/baguio-igorot-stone-kingdom-shut-down-permit-safety-issues/?utm_medium=Social&utm_source=Twitter#Echobox=1668039275-2\n",
      "dupeurl https://www.gmanetwork.com/news/topstories/metro/850920/makati-rated-beyond-compliant-in-gawad-kalasag-disaster-risk-reduction-assessment/story/\n",
      "dupeurl https://thehill.com/homenews/3726972-fox-newss-marc-thiessen-says-midterms-a-disaster-for-gop/\n",
      "dupeurl https://www.rebeccainnesconsulting.com/#services\n",
      "dupeurl https://www.undrr.org/publication/global-assessment-report-disaster-risk-reduction-2022\n",
      "dupeurl https://www.rappler.com/life-and-style/travel/baguio-igorot-stone-kingdom-shut-down-permit-safety-issues/?utm_medium=Social&utm_source=Twitter#Echobox=1667986349-2\n",
      "dupeurl https://app.pinkun.com/23109473/content.html\n",
      "dupeurl https://www.facebook.com/IOMIraq/posts/pfbid0dvshEqgQBiuTrE7TUpABdq9wYMJPDRZ8zUFU7EqzB7PfYoepoPNF5Wa3yjLjySbpl\n",
      "dupeurl https://www.nzherald.co.nz/business/rebuilding-better-once-world-class-nzs-education-system-is-now-a-disaster-how-do-we-fix-it/SFMT6U2WSFGDLABJSL2RHPCR5I/\n",
      "dupeurl https://tinyurl.com/5ypc257b\n",
      "dupeurl https://www.fema.gov/disaster/coronavirus/data-resources/initial-assessment-report/key-findings-recommendations\n",
      "dupeurl http://wrld.bg/qkoc50LpzUp\n",
      "dupeurl http://dlvr.it/ScQYwg\n",
      "dupeurl https://www.powerlineblog.com/archives/2022/11/a-terrible-idea-whose-time-has-come.php?utm_source=twitter&utm_medium=sw&utm_campaign=sw\n",
      "dupeurl https://emergency.copernicus.eu/EMSN137\n",
      "dupeurl https://www.theguardian.com/environment/2022/apr/04/its-over-for-fossil-fuels-ipcc-spells-out-whats-needed-to-avert-climate-disaster\n",
      "dupeurl http://bit.ly/2YslEp3\n",
      "dupeurl https://www.climatelinks.org/resources/usaid-climate-resilience-and-disaster-recovery-capacity-assessment-caribbean-region\n",
      "dupeurl http://princeedwardisland.ca/en/service/provincial-disaster-financial-assistance-program\n",
      "dupeurl https://disasteravoidanceexperts.com/subscribe\n",
      "dupeurl https://express-press-release.net/news/2022/11/07/1287800\n",
      "dupeurl https://cstu.io/bd0f87\n",
      "language issue qme\n",
      "dupeurl https://www.dawn.com/news/1719354/pm-shehbaz-arrives-in-egypt-to-attend-climate-change-summit\n",
      "dupeurl https://bit.ly/3NFJ9zR\n",
      "dupeurl https://www.scienceinter.com/2022/11/climate-crisis-past-eight-years-were.html\n",
      "dupeurl https://ift.tt/3eNy9fq\n",
      "dupeurl https://ift.tt/QoFPuVL\n",
      "dupeurl https://www.thescottishsun.co.uk/sport/football/9723462/celtic-marian-shved-ruthless-neil-lennon-assessment/?utm_medium=Social&utm_campaign=ScottishSunSportTwitter&utm_source=Twitter#Echobox=1667765918\n",
      "dupeurl https://cengizadabag.org/2022/11/ex-celtic-ace-marian-shved-gives-ruthless-neil-lennon-assessment-and-claims-second-season-disaster-showed-his-skills/\n",
      "dupeurl https://uberturco.com/ex-celtic-ace-marian-shved-gives-ruthless-neil-lennon-assessment-and-claims-second-season-disaster-showed-his-skills/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 48 no urls,            51 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_disaster assessment_en.json | Record Count | 75\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster blockchain en yielded 37 results.\n",
      "attempting to write 37 records to google sheets.\n",
      "dupeurl https://www.forbes.com/sites/hershshefrin/2022/11/13/avoiding-the-next-ftx-type-disaster/\n",
      "dupeurl https://vrdigitalminds.com/2022/11/blockchain-gaming-dapps-nonetheless-setting-up-no-matter-ftx-crypto-catastrophe-%e2%8b%86-cryptopreneurss/\n",
      "dupeurl https://www.bocvip.com/265473/shocking-662m-theft-today-crypto-fraud-ftx-exposed/\n",
      "dupeurl http://Blockchain.com\n",
      "dupeurl https://blogo-news.com/ftx-bankruptcy-concerns/\n",
      "dupeurl https://www.linkedin.com/posts/hitoshikokumai_democracy-privacy-ethics-activity-6995642013626880000-cik_\n",
      "dupeurl https://dailycrypto.live/blockchain-gaming-dapps-nonetheless-constructing-regardless-of-ftx-crypto-disaster/\n",
      "dupeurl https://news.cryptokingdomz.com/ftx-ftx-us-and-alameda-file-for-chapter-11-chapter/?feed_id=27303&_unique_id=636e7c7aab1d8\n",
      "dupeurl https://news.cryptokingdomz.com/no-funds-from-tether-as-ftx-faces-9b-liquidity-disaster/?feed_id=27294&_unique_id=636e33fa4448b\n",
      "dupeurl http://opensea.io/Maximilian_Lex\n",
      "dupeurl https://ift.tt/ZLkrCXD\n",
      "dupeurl https://cryptonewswire.com/ftx-disaster-doesn-t-scare-cathie-wood-s-ark-as-they-double-down-on-coinbase-coin\n",
      "dupeurl https://www.50shadesofgreen.com/2022/02/15/bitcoin-can-reverse-americas-disaster-fatigue/\n",
      "dupeurl https://at.tumblr.com/hitoshikokumai/we-live-in-analogdigital-fused-age/3nrvcndewh0x\n",
      "dupeurl https://cryptopotato.com/alameda-ftx-ventures-websites-down-following-ftx-disaster/\n",
      "dupeurl https://www.videogameschronicle.com/news/fifas-first-post-ea-games-will-be-blockchain-titles/\n",
      "dupeurl https://www.linkedin.com/posts/hitoshikokumai_democracy-privacy-ethics-activity-6995642013626880000-cik_\n",
      "dupeurl https://www.coindesk.com/layer2/2022/10/28/save-billions-by-using-the-blockchain-to-distribute-federal-disaster-relief-money\n",
      "dupeurl https://arstechnica.com/tech-policy/2018/11/blockchain-based-elections-would-be-a-disaster-for-democracy/\n",
      "dupeurl https://at.tumblr.com/hitoshikokumai/we-live-in-analogdigital-fused-age/3nrvcndewh0x\n",
      "dupeurl https://www.coindesk.com/layer2/2022/10/28/save-billions-by-using-the-blockchain-to-distribute-federal-disaster-relief-money/?utm_medium=referral&utm_source=rss&utm_campaign=headlines\n",
      "dupeurl https://at.tumblr.com/hitoshikokumai/we-live-in-analogdigital-fused-age/3nrvcndewh0x\n",
      "dupeurl https://cannabisnft.art/d31a8520\n",
      "dupeurl https://www.linkedin.com/posts/hitoshikokumai_democracy-privacy-ethics-activity-6994467187491045376-0q9h\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 26 no urls,            24 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_disaster blockchain_en.json | Record Count | 37\n",
      "\n",
      "\n",
      "\n",
      "disaster response en yielded 100 results.\n",
      "attempting to write 100 records to google sheets.\n",
      "no individual author match on 49252690 ... trying twitter\n",
      "dupeurl https://buff.ly/3WVhr6r\n",
      "dupeurl https://www.nytimes.com/2022/11/10/magazine/natural-disaster-rebuild.html\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://www.theatlantic.com/ideas/archive/2022/10/covid-response-forgiveness/671879/\n",
      "dupeurl https://www.routledge.com/International-and-Local-Actors-in-Disaster-Response-Responding-to-the-Beirut/Haddad/p/book/9781032119908\n",
      "language issue zxx\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://visayas.politics.com.ph/vm-jalandoni-proud-as-talisay-drrm-tram-wins-gawad-kalasag/\n",
      "dupeurl http://bit.ly/3WBr8ac\n",
      "dupeurl https://www.v-20.org/events/the-global-shield-against-climate-risks-enhancing-the-global-disaster-risk-finance-architecture-high-level-panel-discussion-at-the-cvf-v20-pavilion\n",
      "dupeurl https://thehill.com/homenews/administration/3522080-trump-releases-12-page-response-to-jan-6-hearing/\n",
      "dupeurl https://www.nknews.org/koreapro/2022/11/south-koreas-poor-response-to-crowd-crush-tragedy-faces-international-criticism/\n",
      "dupeurl http://www.siswp.org\n",
      "dupeurl https://www.amazon.com/dp/B00C6SITOS?tag=gregg039-20\n",
      "dupeurl https://www.eventbrite.com/e/464222541117\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://www.foxnews.com/us/dallas-airshow-disaster-caught-video-planes-collide-mid-air\n",
      "dupeurl https://bit.ly/3teToSi\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl http://secure.everyaction.com/521PTTsun0ie7uIeFovFAw2\n",
      "dupeurl https://trib.al/ciQhOq0\n",
      "dupeurl https://bit.ly/3DU46CK\n",
      "dupeurl https://mb.com.ph/2022/11/12/silang-lgu-purchases-new-ambulances-other-govt-vehicles-for-emergency-response/\n",
      "dupeurl https://ptvnews.ph/pbbm-thanks-japan-for-assisting-filipinos-providing-pandemic-aid/\n",
      "dupeurl https://mol.im/a/11419945\n",
      "dupeurl https://bit.ly/3DC0y8i\n",
      "dupeurl https://tinyurl.com/4kmxfden\n",
      "dupeurl https://www.pna.gov.ph/articles/1188421\n",
      "dupeurl https://pepsico.shp.so/a/N3CtJc\n",
      "dupeurl https://www.thenews.com.pk/print/1008586-sindh-floods-and-rural-economy\n",
      "dupeurl https://vivegamnews.com/flashnews/state-disaster-response-force-dgp-shailendra-babu-informed/\n",
      "dupeurl https://www.koreatimes.co.kr/www/nation/2022/11/281_339601.html\n",
      "dupeurl https://essayquest.net/incident-response-amp-disaster-recovery-we-have-solved-this-and-many-others-click-it/\n",
      "dupeurl https://turkiyenews.com/5-new-eruptions-occurred-at-popocatepetl-volcano-in-mexico/\n",
      "dupeurl https://vivegamnews.com/flashnews/state-disaster-response-force-on-alert-after-torrential-rains/\n",
      "language issue und\n",
      "dupeurl https://kamadenu.hindutamil.in/national/disaster-response-team-ready-due-to-heavy-rain-warning-police-department-will-also-demand-a-hand-tamil-nadu-dgp-information\n",
      "dupeurl https://www.hindutamil.in/news/tamilnadu/895810-state-disaster-response-force-on-standby.html\n",
      "language issue zxx\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://www.thehindu.com/news/national/tamil-nadu/ndrf-deploys-teams-in-four-districts-that-received-heavy-rain-alert/article66124849.ece\n",
      "dupeurl https://trusa.co/VeteransDay\n",
      "dupeurl https://mb.com.ph/2022/11/11/revilla-hontiveros-extend-aid-to-paengs-victims-in-zamboanga-city/\n",
      "dupeurl http://cod82.prod.acquia-sites.com/governor/news/9056-governor-polis-takes-action-response-covid-19-rsv-influenza-and-other-respiratory\n",
      "dupeurl https://sign.moveon.org/petitions/pakistanis-need-climate-disaster-relief-request-grant-tps-and-ssr-before-it-s-too-late\n",
      "dupeurl https://gopiswrong.net/\n",
      "dupeurl https://www.usaspending.gov/disaster/covid-19?publicLaw=all\n",
      "dupeurl https://mol.im/a/11417715\n",
      "dupeurl https://ncpolicywatch.com/2022/11/10/after-intense-public-scrutiny-about-hurricane-disaster-response-ivan-duncan-a-top-official-at-rebuild-nc-is-resigning/?eType=EmailBlastContent&eId=6f0e7c80-0eab-4366-86bd-694cb78d8941\n",
      "dupeurl http://ournews.ga/lahore/us-partnership-with-punjab-disaster-response/\n",
      "dupeurl http://fema.gov/disaster/4673\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://bit.ly/3fYuiEu\n",
      "dupeurl https://fb.watch/gKjRKWbR7U/\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://gopiswrong.net/\n",
      "dupeurl https://rdcrss.org/3h8TXui\n",
      "dupeurl https://doggett.house.gov/media/blog-post/timeline-trumps-coronavirus-responses\n",
      "dupeurl https://sidewithlove.org/ourstories/2022/11/9/three-ways-we-can-advocate-for-climate-justice-in-november\n",
      "dupeurl https://spr.ly/6012MI6gu\n",
      "dupeurl https://www.instagram.com/p/Ck0920-SSSX/?igshid=YTgzYjQ4ZTY=\n",
      "dupeurl https://pepsico.shp.so/a/pVwdWz\n",
      "dupeurl https://bleuwire.com/best-security-policies-company-should-have/\n",
      "dupeurl https://bit.ly/3E36FSX\n",
      "dupeurl https://fal.cn/3twAt\n",
      "dupeurl https://fal.cn/3twAq\n",
      "dupeurl https://fal.cn/3twAp\n",
      "dupeurl https://fal.cn/3twAo\n",
      "dupeurl https://twilio.smh.re/6nK\n",
      "dupeurl https://vermontdailychronicle.com/tucker-the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://imprimis.hillsdale.edu/the-economic-disaster-of-the-pandemic-response/\n",
      "dupeurl https://indiacsrsummit.in/\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 62 no urls,            69 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_disaster response_en.json | Record Count | 100\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disaster openstreetmap en yielded 1 results.\n",
      "attempting to write 1 records to google sheets.\n",
      "dupeurl http://Disaster.Ninja\n",
      "No new content to add\n",
      "AT completion of write to google sheets, stats are 0 tweet id dupes, 0 no urls,            1 url dupes, 0 new\n",
      "2022-11-13 12:46:15 | test_twitter_data/staged_tweets/2022-11-13_12-46-15_disaster openstreetmap_en.json | Record Count | 1\n",
      "2022-11-13 12:46:15 | | Total Results |  342\n"
     ]
    }
   ],
   "source": [
    "#override for testing\n",
    "#queries = ['openstreetmap']\n",
    "#stop_after = 1\n",
    "\n",
    "i=0\n",
    "while i != stop_after:\n",
    "  \n",
    "    i += 1\n",
    "    total_result_count=0\n",
    "    runDate = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    runDate_file_format = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    queries = refresh_queries(control_input)\n",
    "    for query in queries:\n",
    "        for language in search_languages:\n",
    "            print('\\n\\n')\n",
    "            translation = translator.translate(query, src=\"en\", dest=language)\n",
    "            search = tweetQueryBuilder(translation.text)\n",
    "\n",
    "            folder = 'test_twitter_data/staged_tweets/'\n",
    "\n",
    "            #search = search + '&result_type=popular'\n",
    "            #print(search)\n",
    "            response = requests.get(search, headers={\"Authorization\":bearer})\n",
    "\n",
    "\n",
    "            parsed = json.loads(response.text)\n",
    "\n",
    "            # get results cout\n",
    "            rc = parsed['meta']['result_count']\n",
    "            print(f'{query} {language} yielded {rc} results.')\n",
    "            \n",
    "            #if at least one result was returned\n",
    "            if rc > 0:\n",
    "                \n",
    "                \n",
    "                \n",
    "                ##check for hard dupe\n",
    "                #df_already_saved_tweets = pd.DataFrame(tweet_list.get_all_records()) #seed the list with one row of data\n",
    "                \n",
    "                ##check for URL\n",
    "                ##get author\n",
    "                ##get content\n",
    "                ##persist data\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(f'attempting to write {rc} records to google sheets.')\n",
    "                write_results_to_google_sheets(parsed) #export to google sheets\n",
    "            \n",
    "            total_result_count += rc\n",
    "\n",
    "            #print(json.dumps(parsed, indent=4, sort_keys=True))\n",
    "\n",
    "            fullFilePath = folder + runDate_file_format + '_' + query + '_' + language +'.json'\n",
    "            #print(fullFilePath)\n",
    "            print(f\"{runDate} | {fullFilePath} | Record Count | {str(rc)}\")\n",
    "\n",
    "            with open(fullFilePath, 'w') as outfile:\n",
    "                outfile.write(json.dumps(parsed, indent=4, sort_keys=True))\n",
    "    print(f\"{runDate} | | Total Results |  {total_result_count}\")\n",
    "    \n",
    "    time.sleep(sleeptime)\n",
    "    \n",
    "    #time.sleep(24*60*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
