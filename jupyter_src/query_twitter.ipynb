{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test or Prod\n",
    "env = 'prod'  #'prod'\n",
    "\n",
    "if env == 'test':\n",
    "    RESULTS_WORKBOOK = {'sheet_id' : '1HAqjdiUgvDK-0sQjsg-TYEIxJf2q0WD7X8XUs_pPOx0',\n",
    "                     'range_names': ['staged_tweets','twitter_authors']}   \n",
    "    stop_after = 1\n",
    "    sleeptime = 1\n",
    "    search_languages = ['en']\n",
    "    control_input = 'googlesheets'\n",
    "elif env == 'prod':\n",
    "    RESULTS_WORKBOOK = {'sheet_id' : '1_6-O1D7UtbA4PiDNr71Fm-kA_OJk8S7RBctkGjZRIVk',\n",
    "                     'range_names': ['tweet_list','staged_tweets','twitter_authors']}\n",
    "    stop_after = -1\n",
    "    sleeptime = 36000 #10 hour sleep time\n",
    "    search_languages = ['en']\n",
    "    control_input = 'googlesheets'\n",
    "else:\n",
    "    print(\"\"\"'valid env values are 'prod' or 'test'\"\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "\n",
    "#https://www.thepythoncode.com/article/translate-text-in-python\n",
    "from googletrans import Translator, constants\n",
    "from pprint import pprint\n",
    "\n",
    "#to connect to google\n",
    "from google.oauth2 import service_account\n",
    "import pygsheets\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "import jmespath\n",
    "import urlexpander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_twitter_version = 'v2022-11-26'\n",
    "runDate = datetime.today().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "#twitter variables\n",
    "API_Key = os.environ.get(\"API_KEY\")\n",
    "API_Key_Secret = os.environ.get(\"API_KEY_SECRET\")\n",
    "Bearer_Token = os.environ.get(\"BEARER_TOKEN\")\n",
    "bearer = 'bearer ' + Bearer_Token\n",
    "\n",
    "\n",
    "#google variables\n",
    "SERVICE_ACCOUNT_FILE = os.environ.get(\"GOOGLE_SERVICE_ACCOUNT_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheets Variables\n",
    "# variables that shouldn't change by person\n",
    "\n",
    "SEARCH_WORKBOOK = {'sheet_id' : '1QjvZOnkCJM-BcRvMlP0XsN0hJeanKPiK7_e37uJiKek',\n",
    "                    'range_names': ['twitter_keywords','stop_phrases']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_google_sheets_connector():\n",
    "    # Set up Google Credentials\n",
    "    #SERVICE_ACCOUNT_FILE = google_service_account_file\n",
    "\n",
    "    SCOPES = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    # connect\n",
    "    gc = pygsheets.authorize(credentials=service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, \n",
    "        scopes=SCOPES))\n",
    "    \n",
    "    return gc\n",
    "\n",
    "gc = instantiate_google_sheets_connector()\n",
    "\n",
    "#load to workbook objects\n",
    "search_workbook = gc.open_by_key(SEARCH_WORKBOOK['sheet_id'])\n",
    "results_workbook = gc.open_by_key(RESULTS_WORKBOOK['sheet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate read-only dataframes\n",
    "keywords = search_workbook[0]\n",
    "#df_r_keywords = pd.DataFrame(keywords.get_all_records())\n",
    "\n",
    "stops = search_workbook[1]\n",
    "#df_r_stop_phrases = pd.DataFrame(stop_phrases.get_all_records())\n",
    "\n",
    "#generate updatable dataframes\n",
    "tweet_list = results_workbook[0]\n",
    "#df_u_tweet_list = pd.DataFrame(tweet_list.get_all_records())\n",
    "\n",
    "staged_tweets = results_workbook[1]\n",
    "#df_u_staged_tweets = pd.DataFrame(staged_tweets.get_all_records())\n",
    "\n",
    "twitter_authors = results_workbook[2]\n",
    "global DF_AUTHORS\n",
    "DF_AUTHORS = pd.DataFrame(twitter_authors.get_all_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twitter_scanner:\n",
    "    def __init__(self, bearer, keywords):\n",
    "        self.bearer = bearer\n",
    "        self.base_api_url = 'https://api.twitter.com/2/tweets/search/recent?query='\n",
    "        self.fixed_args = '-is:retweet  has:links '\n",
    "        self.query_arg_suffix = '&max_results=100&tweet.fields=attachments,author_id,context_annotations,\\\n",
    "created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,\\\n",
    "referenced_tweets,source,text,withheld&expansions=referenced_tweets.id,geo.place_id'\n",
    "        self.refresh_scan_terms()\n",
    "\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.query_arg_suffix}'\n",
    "\n",
    "    def scan_for_search_terms(self,keywords):\n",
    "        print(f'scanning for {keywords}')\n",
    "        query_arg_prefix = self.fixed_args + keywords\n",
    "        query_arg_prefix = urllib.parse.quote(query_arg_prefix)\n",
    "        query = self.base_api_url + query_arg_prefix + self.query_arg_suffix\n",
    "        \n",
    "        response = requests.get(query, headers={\"Authorization\":bearer})\n",
    "        return(json.loads(response.text))\n",
    "        \n",
    "    \n",
    "    def scan_for_author(self,author):\n",
    "        print(f'scanning for {author}')\n",
    "        query_arg_prefix = urllib.parse.quote(f'-is:retweet  has:links from:{author}')   \n",
    "        query = self.base_api_url + query_arg_prefix + self.query_arg_suffix\n",
    "\n",
    "        \n",
    "        response = requests.get(query, headers={\"Authorization\":bearer})\n",
    "        return(json.loads(response.text))\n",
    "    \n",
    "    def refresh_scan_terms(self):\n",
    "        self.keywords = pd.DataFrame(keywords.get_all_records())\n",
    "        self.keywords.status = self.keywords.status.str.lower()\n",
    "        self.keywords = self.keywords[self.keywords.status == 'active']\n",
    "\n",
    "    \n",
    "    def scan_all_active_terms(self,num,sleep_time):\n",
    "        i = 0\n",
    "        while i != num:\n",
    "            i+=1\n",
    "            self.refresh_scan_terms()\n",
    "            key_terms = self.keywords.key_words[self.keywords.record_type == 'search_term'].tolist()\n",
    "            authors = self.keywords.key_words[self.keywords.record_type == 'twitter_user'].tolist()\n",
    "            \n",
    "            for t in key_terms:\n",
    "                res = self.scan_for_search_terms(t)\n",
    "                df = tf.tweets_to_df(res)\n",
    "                df['query_twitter_version'] = query_twitter_version\n",
    "                df['search_term'] = f'search_string:{t}'\n",
    "                ts.screen_tweets(df)\n",
    "\n",
    "            for a in authors:\n",
    "                res = self.scan_for_author(a)\n",
    "                df = tf.tweets_to_df(res)\n",
    "                df['query_twitter_version'] = query_twitter_version\n",
    "                df['search_term'] = f'author:{a}'               \n",
    "                ts.screen_tweets(df)            \n",
    "    \n",
    "            print(f'about to sleep for {sleep_time}, i is {i}')\n",
    "            time.sleep(sleep_time)\n",
    "            print('done')\n",
    "                  \n",
    "    \n",
    "tscanner = twitter_scanner(bearer, keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_screener:\n",
    "    def __init__(self, stops, staged_tweets):\n",
    "\n",
    "        self.staged_tweets = pd.DataFrame(staged_tweets.get_all_records())\n",
    "        self.stops = pd.DataFrame(stops.get_all_records())\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.stops}'\n",
    "\n",
    "    \n",
    "    def refresh_stops(self):\n",
    "        #print('refreshing critera')\n",
    "        self.stops = pd.DataFrame(stops.get_all_records())\n",
    "        self.stops = self.stops[self.stops.type == 'text']\n",
    "        print(self.stops)\n",
    "        \n",
    "    def refresh_saved_tweets(self):\n",
    "        #print('refreshing saved tweets')\n",
    "        self.staged_tweets = pd.DataFrame(staged_tweets.get_all_records())\n",
    "        \n",
    "    def screen_tweets(self, df_tweets):\n",
    "        self.refresh_saved_tweets()\n",
    "        #self.df_tweets = df_tweets\n",
    "        \n",
    "        #check to see if tweets have already been saved\n",
    "         \n",
    "        \n",
    "        #print(f'before checking for dupes {df_tweets.shape}')\n",
    "        df_tweets = df_tweets.assign(tweet_already_staged=df_tweets.tweet_id.isin(self.staged_tweets.tweet_id).astype(int))\n",
    "        df_x = df_tweets[df_tweets['tweet_already_staged'] == 0]\n",
    "        #print(f'after checking for dupes {df_x.shape}')\n",
    "        self.stage_tweets(df_x)\n",
    "        \n",
    "        return df_tweets\n",
    "    \n",
    "    def stage_tweets(self, df_tweets):\n",
    "        self.df_tweets = df_tweets\n",
    "        \n",
    "        #if anything remains, write to the tweet_list sheet\n",
    "        if len(df_tweets) > 0:\n",
    "            staged_tweets.append_table(df_tweets.values.tolist(), start='A1', end=None, dimension='ROWS', overwrite=False)\n",
    "   \n",
    "        #else:\n",
    "            #print('No new content to add')\n",
    "            \n",
    "    def get_staged_tweets(self):\n",
    "        return self.staged_tweets\n",
    "\n",
    "    \n",
    "ts = tweet_screener(stops, staged_tweets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_formatter:\n",
    "    def __init__(self):\n",
    "        self\n",
    "        # init the Google API translator\n",
    "        self.translator = Translator()\n",
    "    \n",
    "    def tweets_to_df(self,json):\n",
    "\n",
    "        \n",
    "        cols = ['create_date','create_time','tweet_url','tweet_id','author_username',\\\n",
    "                'reference_url','text','orig_lang','orig_text']\n",
    "        df_tweets = pd.DataFrame(columns=cols)\n",
    "        \n",
    "        if json['meta']['result_count'] == 0:\n",
    "            #print('no results')\n",
    "            return df_tweets # return an empty df\n",
    "        \n",
    "        \n",
    "        for tweet in json['data']:\n",
    "            \n",
    "            tweet_id = 'tweet_id:' + str(jmespath.search(\"id\", tweet))\n",
    "            #print(tweet_id)\n",
    "            urls = jmespath.search(\"entities.urls[*].expanded_url\", tweet)\n",
    "            primary_url = self.screen_urls(urls)\n",
    "            \n",
    "            ## if there's no primary url, don't bother with the rest\n",
    "            if primary_url == 'none found':\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            \n",
    "            text = jmespath.search(\"text\", tweet)\n",
    "            orig_text = text\n",
    "            text = self.scrub_text(text)\n",
    "            original_lang = jmespath.search(\"lang\", tweet)\n",
    "            if original_lang != 'en':\n",
    "                try:\n",
    "                    #print(f'translating from {original_lang} -- {text}')\n",
    "                    translation = self.translator.translate(text, src=original_lang, dest='en')\n",
    "                    text = translation.text\n",
    "                except:\n",
    "                    print(f'translation issue {original_lang}')\n",
    "            \n",
    "            created_date = jmespath.search(\"created_at\", tweet)\n",
    "            create_date, create_time = created_date.split('T')\n",
    "            create_time = create_time[:8]\n",
    "            \n",
    "            author_id = jmespath.search(\"author_id\", tweet)\n",
    "            author_username = self.get_author_info(author_id)\n",
    "            \n",
    "            tweet_url = f'https://twitter.com/{author_username}/status/{tweet_id}'\n",
    "            \n",
    "            tweet_info = [create_date, create_time, tweet_url, tweet_id, author_username, primary_url, text, original_lang, orig_text ]\n",
    "\n",
    "            df_tweets.loc[len(df_tweets)] = tweet_info\n",
    "            \n",
    "            df_tweets = df_tweets.drop_duplicates(subset='reference_url')\n",
    "            \n",
    "            \n",
    "        return df_tweets\n",
    "            \n",
    "    def scrub_text(self,t):\n",
    "    \n",
    "        if isinstance(t, str):\n",
    "            #remove newlines\n",
    "            t = re.sub('\\n', ' ', t)\n",
    "            #remove amp\n",
    "            t = re.sub('&amp;', ' and ', t)\n",
    "            #remove pipes because I use them as delimiters\n",
    "            t = t.replace('|', ' ')  \n",
    "            #remove unicode special chars\n",
    "            string_encode = t.encode(\"ascii\", \"ignore\")\n",
    "            t = string_encode.decode()\n",
    "\n",
    "        return t\n",
    "    \n",
    "    def screen_urls(self,urls):\n",
    "        #capture urls\n",
    "        patterns = [re.compile(\"pic\\.twitter\\.com\"), \n",
    "                    re.compile(\"twitter\\.com\"),\n",
    "                    re.compile(\"amzn\\.\"),\n",
    "                    re.compile(\"youtube\\.\"),\n",
    "                    re.compile(\"linkedin\\.\"),\n",
    "                   ]\n",
    "        returnUrls = []\n",
    "\n",
    "        for url in urls:\n",
    "            include = 'Y'\n",
    "            for pattern in patterns:\n",
    "                if pattern.search(url) != None:\n",
    "                    include = 'N'\n",
    "            if include == 'Y':\n",
    "                returnUrls.append(url)\n",
    "                \n",
    "        main_url = 'none found'      \n",
    "        if len(returnUrls) > 0:\n",
    "            main_url = returnUrls[0]\n",
    "\n",
    "        return main_url\n",
    "    \n",
    "    \n",
    "    def get_author_info(self,author_id):\n",
    "        global DF_AUTHORS\n",
    "        global runDate\n",
    "        #add the \"TID\"\n",
    "        internal_author_id = 'TID' + author_id\n",
    "\n",
    "        auth_list = DF_AUTHORS[DF_AUTHORS['author_id'] == internal_author_id].values.tolist()\n",
    "        if len(auth_list) == 1:\n",
    "            where = 'locally'\n",
    "            author_username = auth_list[0][3]\n",
    "\n",
    "        elif len(auth_list) > 1:\n",
    "            author_username = 'UNKN_MultipleResultsReturned'\n",
    "\n",
    "        else:\n",
    "            print(f'no individual author match on {author_id} ... trying twitter')\n",
    "\n",
    "            userFields = 'location,url,description,entities'\n",
    "            api_getAuthor = f'https://api.twitter.com/2/users?ids={author_id}&user.fields={userFields}'\n",
    "            response = requests.get(api_getAuthor, headers={\"Authorization\":bearer})\n",
    "            author_from_twitter = json.loads(response.text)\n",
    "            time.sleep(1) #throttle volume\n",
    "            #print(f' found {author_from_twitter}')\n",
    "\n",
    "            if 'data' in author_from_twitter:\n",
    "                where = 'twitter'\n",
    "\n",
    "                author_name = author_from_twitter['data'][0]['name']\n",
    "                author_username = author_from_twitter['data'][0]['username']\n",
    "                try:\n",
    "                    author_description = author_from_twitter['data'][0]['description']\n",
    "                except:\n",
    "                    author_description = ''\n",
    "                try:\n",
    "                    author_url = author_from_twitter['data'][0]['url']\n",
    "                except:\n",
    "                    author_url = ''\n",
    "\n",
    "            elif 'errors' in   author_from_twitter:\n",
    "                author_name = author_from_twitter['errors'][0]['title']\n",
    "                author_username = author_from_twitter['errors'][0]['title']\n",
    "                author_description = author_from_twitter['errors'][0]['detail']\n",
    "                author_url = author_from_twitter['errors'][0]['type']\n",
    "\n",
    "            else:\n",
    "                author_username = 'UNKNN_NoResultsReturned'\n",
    "\n",
    "\n",
    "\n",
    "            # add values to google sheets\n",
    "            vals= [runDate, internal_author_id, author_name, author_username, author_description, author_url]\n",
    "\n",
    "            #print(f'before cleanup of {vals}')\n",
    "            #clean up text \n",
    "            for i, item in enumerate(vals):\n",
    "                vals[i] = self.scrub_text(item)\n",
    "            #print(f'after cleanup of {vals}')\n",
    "\n",
    "            twitter_authors.append_table(vals, start='A1', end=None, dimension='ROWS', overwrite=False)\n",
    "\n",
    "            # add values to DF_AUTHORS\n",
    "            r = pd.Series(vals, index = DF_AUTHORS.columns)\n",
    "            DF_AUTHORS = DF_AUTHORS.append(r, ignore_index=True)\n",
    "\n",
    "\n",
    "        return author_username\n",
    "\n",
    "    \n",
    "\n",
    "#assert get_author_info('2576444334') == 'TheMissingMaps' #positive use-case\n",
    "#assert get_author_info('1477759062884421631') == 'Not Found Error' #negative use-case\n",
    "\n",
    "\n",
    "#assert scrub_text('asdfa|sdf &amp; dsdfgsdfg') == 'asdfa sdf  and  dsdfgsdfg'\n",
    "tf = tweet_formatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscanner.scan_all_active_terms(stop_after,sleeptime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
