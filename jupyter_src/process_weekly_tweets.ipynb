{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop phrases\n",
    "'join us'\n",
    "'days left'\n",
    "'mark your calendars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "#import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from googletrans import Translator, constants\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import jmespath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')\n",
    "weekly_filename = f'deduped_weekly_tweets_{date}.json'\n",
    "URL_DUPES={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSavedMappings():\n",
    "    \n",
    "    AUTHOR_MAP = {}\n",
    "    \n",
    "    #author mappings\n",
    "    map_file = 'authors.json'\n",
    "    \n",
    "    fullFilePath = f'twitter_data/saved_mappings/{map_file}'\n",
    "    with open(fullFilePath, 'r') as f:\n",
    "        AUTHOR_MAP = json.load(f)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return AUTHOR_MAP\n",
    "\n",
    "AUTHOR_MAP = loadSavedMappings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_author(author_id):\n",
    "    \n",
    "    \n",
    "    auth_list = jmespath.search(f\"data[?id == '{author_id}'].username\", AUTHOR_MAP)\n",
    "\n",
    "\n",
    "    if len(auth_list) == 1:\n",
    "        author_username = auth_list[0]\n",
    "    elif len(auth_list) > 1:\n",
    "        author_username = 'UNKN_MultipleResultsReturned'\n",
    "    else:\n",
    "        author_username = author_id\n",
    "\n",
    "\n",
    "    return author_username\n",
    "\n",
    "\n",
    "#test\n",
    "author_username = get_local_author('774273192395280384')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrubText(tweetText):\n",
    "    \n",
    "    #remove newlines\n",
    "    tweetText = re.sub('\\n', ' ', tweetText)\n",
    "    \n",
    "    #remove amp\n",
    "    tweetText = re.sub('&amp;', ' and ', tweetText)\n",
    "    \n",
    "      #remove pipes because I use them as delimiters\n",
    "    tweetText = tweetText.replace('|', ' ')  \n",
    "    \n",
    "    #remove unicode special chars\n",
    "    string_encode = tweetText.encode(\"ascii\", \"ignore\")\n",
    "    tweetText = string_encode.decode()\n",
    "    \n",
    "    return tweetText\n",
    "\n",
    "scrubText('asdfa|sdf &amp; dsdfgsdfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating Varibales -- derived programatically or rarely changed\n",
    "\n",
    "userName = 'defaultUserName'\n",
    "\n",
    "#MAPPING_FILE_DIR = 'twitter_data/saved_mappings/'\n",
    "weekly_file = f'twitter_data/weekly_files/{weekly_filename}'\n",
    "baseTweetURL = 'https://twitter.com/{userName}/status/{tweetId}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_urls(urls):\n",
    "    \n",
    "    #capture urls\n",
    "    patterns = [re.compile(\"pic\\.twitter\\.com\"), \n",
    "                re.compile(\"twitter\\.com\")]\n",
    "    returnUrl = []\n",
    "    \n",
    "    for url in urls:\n",
    "        include = 'Y'\n",
    "        for pattern in patterns:\n",
    "            if pattern.search(url) != None:\n",
    "                include = 'N'\n",
    "        if include == 'Y':\n",
    "            returnUrl.append(url)\n",
    "    \n",
    "    return returnUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_url_dupes(url):\n",
    "    dupe = 0\n",
    "    if url in URL_DUPES:\n",
    "        URL_DUPES[url] += 1\n",
    "        dupe = 1\n",
    "        \n",
    "    else:\n",
    "        URL_DUPES[url] = 0\n",
    "        \n",
    "    return dupe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_DUPES = {}\n",
    "iterr = 0\n",
    "rollingContent = ''\n",
    "summaryDF = []\n",
    "\n",
    "weekly_file = f'twitter_data/weekly_files/{weekly_filename}'\n",
    "\n",
    "summaryFilePath = f'twitter_data/weekly_summaries/summary_{date}.txt'\n",
    "with open(summaryFilePath, 'w') as summaryFile:\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    summaryFile.write('reportDate|s_tweetURL|s_referenceURL|tweetId|author|tweetURL|referenceURL|tweetText|PromoteIndicator|Time Sensitive Flag|Use Case|Technical Underpinning|Verbiage')\n",
    "    summaryFile.write(\"\\n\")\n",
    "    \n",
    "    summaryHeaderDF = ['tweetId','author','likes','retweets','tweetURL','tweetText','referenceUrls']\n",
    "    \n",
    "    \n",
    "    with open(weekly_file, 'r') as f:\n",
    "        tweets = json.load(f)\n",
    "\n",
    "\n",
    "        for tweet in tweets['data']:\n",
    "\n",
    "\n",
    "            iterr +=1\n",
    "\n",
    "\n",
    "\n",
    "            #print(tweet)\n",
    "            author = get_local_author(jmespath.search(\"author_id\", tweet))\n",
    "            text = jmespath.search(\"text\", tweet)\n",
    "            text = scrubText(text)\n",
    "            likes = jmespath.search(\"public_metrics.like_count\", tweet)\n",
    "            retweets = jmespath.search(\"public_metrics.retweet_count\", tweet)\n",
    "            urls = jmespath.search(\"entities.urls[*].expanded_url\", tweet)\n",
    "            urls = remove_twitter_urls(urls)\n",
    "\n",
    "      \n",
    "            url_dupe_flag=0\n",
    "            if len(urls) == 0:\n",
    "                url_dupe_flag = 1 #dupe or no url is functionally the same\n",
    "            else:    \n",
    "                url_dupe_flag = check_for_url_dupes(urls[0])\n",
    "\n",
    "            if url_dupe_flag == 1:\n",
    "                continue\n",
    "            \n",
    "            stringUrl =''\n",
    "            firstURL = urls[0]\n",
    "            for url in urls:\n",
    "                stringUrl += '|' + url\n",
    "\n",
    "            tweetId = jmespath.search(\"id\", tweet)\n",
    "\n",
    "            tweetURL = f'https://twitter.com/{author}/status/{tweetId}'\n",
    "\n",
    "            blank=''\n",
    "            content = date + \"|\" + tweetURL+ \"|\"+ firstURL + \"|\" + str(tweetId)+ \"|\"+author+ \"|||\"+text\n",
    "            contentDF = [tweetId,author,tweetURL,text,urls]\n",
    "            summaryDF.append(contentDF)\n",
    "            rollingContent = rollingContent + ' ' + text\n",
    "            summaryFile.write(content)\n",
    "            summaryFile.write(\"\\n\")\n",
    "            #time.sleep(2)\n",
    "    f.close()\n",
    "summaryFile.close()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "new_stopwords = ['https']\n",
    "\n",
    "stop_words.extend(new_stopwords)\n",
    "\n",
    "word_data = rollingContent\n",
    "word_data = word_data.lower()\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "nltk_tokens= [word for word in nltk_tokens if word.isalnum()]\n",
    "\n",
    "\n",
    "\n",
    "filtered_sentence = [w for w in nltk_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in nltk_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "\n",
    "print(filtered_sentence)\n",
    "\n",
    "print(list(nltk.bigrams(nltk_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(filtered_sentence)\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "\n",
    "fdist.most_common(100)\n",
    "\n",
    "#for k,v in fdist.items():\n",
    "#    print (k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist = nltk.FreqDist(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = ['s','d']\n",
    "z = ['a','b']\n",
    "\n",
    "x.append(y)\n",
    "x.append(z)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame (summaryDF, columns = summaryHeaderDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweetText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '.*diamcircle.*'\n",
    "\n",
    "df['lc_tweetText'] = df['tweetText'].str.lower()\n",
    "\n",
    "df[df.lc_tweetText.str.match(regex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL_DUPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
